{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Analyser\n",
    "\n",
    "Output topics contained within free-text.\n",
    "\n",
    "\n",
    "## Citations:\n",
    "\n",
    "(1) Uses spaCy - a free, open-source package for \"Industrial Strength NLP\" (Natural Language Processing)\n",
    "\n",
    "(2) spaCy uses gloVe - Global Vectors for Word Representation Ref: Jeffrey Pennington, Richard Socher, and Christopher D. Manning. 2014. https://nlp.stanford.edu/pubs/glove.pdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed 29 Aug 11:26:24 BST 2018\r\n"
     ]
    }
   ],
   "source": [
    "!date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read sample reasons and all reasons Dataframes from CSV files\n",
    "import pandas as pd\n",
    "import os as os\n",
    "\n",
    "project_fileloc = './'\n",
    "sample_reasons_filename = 'Data_Final_N=14,645_6_17_18_sample_reasons.csv'\n",
    "sample_reasons_filepath = project_fileloc + sample_reasons_filename\n",
    "if os.path.exists(sample_reasons_filepath):\n",
    "    sample_reasons = pd.read_csv(sample_reasons_filepath)\n",
    "else:\n",
    "    print(\"File\", sample_reasons_filepath, \"not found\")\n",
    "    \n",
    "all_reasons_filename = 'Data_Final_N=14,645_6_17_18_all_reasons.csv'\n",
    "all_reasons_filepath = project_fileloc + all_reasons_filename\n",
    "if os.path.exists(all_reasons_filepath):\n",
    "    all_reasons = pd.read_csv(all_reasons_filepath)\n",
    "else:\n",
    "    print(\"File\", all_reasons_filepath, \"not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Install spaCy** - \"Industrial strength NLP\" - to use its Natural Language Processing tools and its gloVe word vectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n#??? un-comment if not already done\\n!pip install spacy\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "#??? un-comment if not already done\n",
    "!pip install spacy\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n#??? if not already done, download all the spaCy model for English - use the large model to get the vectors\\n!python -m spacy download  en_core_web_lg\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "#??? if not already done, download all the spaCy model for English - use the large model to get the vectors\n",
    "!python -m spacy download  en_core_web_lg\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPACY_MODEL = 'en_core_web_lg'   # make sure to use larger model for the word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** WARNING: spaCy model en_core_web_lg takes approx. 2GB of memory\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "print(\"*** WARNING: spaCy model\", SPACY_MODEL, \"takes approx. 2GB of memory\")\n",
    "nlp = spacy.load(SPACY_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "NO_RESPONSE_STR = 'missing_no_answer_given'\n",
    "NOT_YET_CODED_STR = 'missing_not_yet_coded'\n",
    "NOT_APPLICABLE_STR = 'not_applicable'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Artificial words/word-phrases to ignore - as requested by the researchers\n",
    "exclude_words=['junk', NO_RESPONSE_STR, NOT_YET_CODED_STR, NOT_APPLICABLE_STR]\n",
    "capitalized = [w.capitalize() for w in exclude_words]\n",
    "for word in capitalized: exclude_words.append(word)\n",
    "exclude_words_without_stop_words = exclude_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /home/zellsmith2/.conda/envs/py36/lib/python3.6/site-packages (3.3)\n",
      "Requirement already satisfied: six in /home/zellsmith2/.conda/envs/py36/lib/python3.6/site-packages (from nltk) (1.11.0)\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 18.0 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/zellsmith2/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "words that will be excluded (not so useful 'stop-words' and artificial words): ['junk', 'missing_no_answer_given', 'missing_not_yet_coded', 'not_applicable', 'Junk', 'Missing_no_answer_given', 'Missing_not_yet_coded', 'Not_applicable', 'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\", 'I', 'Me', 'My', 'Myself', 'We', 'Our', 'Ours', 'Ourselves', 'You', \"You're\", \"You've\", \"You'll\", \"You'd\", 'Your', 'Yours', 'Yourself', 'Yourselves', 'He', 'Him', 'His', 'Himself', 'She', \"She's\", 'Her', 'Hers', 'Herself', 'It', \"It's\", 'Its', 'Itself', 'They', 'Them', 'Their', 'Theirs', 'Themselves', 'What', 'Which', 'Who', 'Whom', 'This', 'That', \"That'll\", 'These', 'Those', 'Am', 'Is', 'Are', 'Was', 'Were', 'Be', 'Been', 'Being', 'Have', 'Has', 'Had', 'Having', 'Do', 'Does', 'Did', 'Doing', 'A', 'An', 'The', 'And', 'But', 'If', 'Or', 'Because', 'As', 'Until', 'While', 'Of', 'At', 'By', 'For', 'With', 'About', 'Against', 'Between', 'Into', 'Through', 'During', 'Before', 'After', 'Above', 'Below', 'To', 'From', 'Up', 'Down', 'In', 'Out', 'On', 'Off', 'Over', 'Under', 'Again', 'Further', 'Then', 'Once', 'Here', 'There', 'When', 'Where', 'Why', 'How', 'All', 'Any', 'Both', 'Each', 'Few', 'More', 'Most', 'Other', 'Some', 'Such', 'No', 'Nor', 'Not', 'Only', 'Own', 'Same', 'So', 'Than', 'Too', 'Very', 'S', 'T', 'Can', 'Will', 'Just', 'Don', \"Don't\", 'Should', \"Should've\", 'Now', 'D', 'Ll', 'M', 'O', 'Re', 'Ve', 'Y', 'Ain', 'Aren', \"Aren't\", 'Couldn', \"Couldn't\", 'Didn', \"Didn't\", 'Doesn', \"Doesn't\", 'Hadn', \"Hadn't\", 'Hasn', \"Hasn't\", 'Haven', \"Haven't\", 'Isn', \"Isn't\", 'Ma', 'Mightn', \"Mightn't\", 'Mustn', \"Mustn't\", 'Needn', \"Needn't\", 'Shan', \"Shan't\", 'Shouldn', \"Shouldn't\", 'Wasn', \"Wasn't\", 'Weren', \"Weren't\", 'Won', \"Won't\", 'Wouldn', \"Wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "# Add 'stop' words and their capitalisations to the list of words to ignore\n",
    "!pip install nltk\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "for word in stop_words: exclude_words.append(word)\n",
    "capitalized_stop_words = [w.capitalize() for w in stop_words]\n",
    "for word in capitalized_stop_words: exclude_words.append(word)\n",
    "\n",
    "print(\"words that will be excluded (not so useful 'stop-words' and artificial words):\", exclude_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## spaCy class to perform various operations on free-text\n",
    "- spaCy has several powerful features which were not explored in detail in the Fellowship project. These features include Part-Of-Speech tagging (POS) and Dependency Trees to yield a deeper understanding of the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from collections import Counter\n",
    "\n",
    "class SpaCyFreeText():\n",
    "    \"\"\"\n",
    "    Class to perform some spaCy operations on a free-text Panda Series object.\n",
    "\n",
    "    Per recommendation from ASI's Tom Begley, as the size of the text data is not very small, spaCy’s `pipe`\n",
    "    is used to iterate over the text. This improves speed by accumulating a buffer and operating on the text\n",
    "    in parallel. However, Tom adds a note of caution, including with the argument 'n_threads', which appears\n",
    "    to make no difference no matter what it is set to.\n",
    "\n",
    "    Warning: spaCy token.similarity(other) has a bug when 'other' has length 1, producing the\n",
    "    following error:\n",
    "        # \"TypeError: 'spacy.tokens.token.Token' object does not support indexing\"\n",
    "    The work-around used is to ignore tokens with length 1.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, nlp, name, free_text_Series):\n",
    "        \"\"\" Instantiate the class with the specified spaCy nlp object \"\"\"\n",
    "        self.nlp = nlp\n",
    "        self.name = name\n",
    "        \n",
    "        # Add a full-stop to the end of each free-text item if it does not already end in punctuation.\n",
    "        # Replace null entries and remove unnecessary angle-quotes (as requested by Zorana)\n",
    "        self.free_text_list = []\n",
    "        for value in free_text_Series.fillna(NO_RESPONSE_STR).str.replace(\"‘\", \"\").replace(\"’\", \"\"):\n",
    "            last_char = value[-1]\n",
    "            if last_char not in string.punctuation:\n",
    "                value += \".\"\n",
    "            self.free_text_list.append(value)\n",
    "        free_text_Series = pd.Series(self.free_text_list)\n",
    "        \n",
    "        # Create a spaCy document for each item in the free-text\n",
    "        self.doc_list = []\n",
    "        for doc in self.nlp.pipe(free_text_Series):\n",
    "            self.doc_list.append(doc)\n",
    "\n",
    "    def get_name(self): return self.name\n",
    "\n",
    "    def get_free_text_list(self): return self.free_text_list\n",
    "    \n",
    "    def get_doc_list(self): return self.doc_list\n",
    "\n",
    "    def get_most_common_pos(self, pos, top_n = 10, exclude_words=[]):\n",
    "        \"\"\"\n",
    "        Get top_n most common Parts Of Speech (top n nouns, verbs etc.)\n",
    "        Specify pos as 'None' to match all parts-of-speech\n",
    "        Specify top_n as 'None' to print all words\n",
    "        Use exclude_words to ignore certain words, e.g. not so useful 'stop words' or artificial words.\n",
    "        \"\"\"\n",
    "        word_list = []\n",
    "        for doc in self.doc_list:\n",
    "            for token in doc:\n",
    "                # Allow for matching multiple POS, e.g. \"NOUN,PROPN\" or all POS if pos=None\n",
    "                if not pos or token.pos_ in pos.split(\",\"):\n",
    "                    token_no_end_punc = token.lower_.rstrip(string.punctuation)\n",
    "                    if token_no_end_punc not in exclude_words:\n",
    "                        word_list.append(token_no_end_punc)\n",
    "                    # ??? TODO: make use of lower_ and rstrip configurable?\n",
    "        most_common_pos = Counter(word_list).most_common(top_n)\n",
    "        return(most_common_pos)\n",
    "\n",
    "    def print_most_common_pos(self, pos, top_n = 10, exclude_words=[]):\n",
    "        if top_n:\n",
    "            print(\"Top\", top_n, end='')\n",
    "        else:\n",
    "            print(\"All\", end='')\n",
    "        print(\" \" + pos + \"s for\", self.name)\n",
    "        most_common_pos = self.get_most_common_pos(pos, top_n, exclude_words)\n",
    "        print(most_common_pos)\n",
    "        \n",
    "    def print_most_common_pos_with_words_before(self, pos, top_n = 10, exclude_words=[]):\n",
    "        \"\"\"\n",
    "        Print top_n words with the specified part-of-speech and their preceeding word (if any).\n",
    "        Specify pos as 'None' to match all parts-of-speech\n",
    "        Specify top_n as 'None' to print all words\n",
    "        Use exclude_words to ignore certain words, e.g. not so useful 'stop words' or artificial words.\n",
    "        \"\"\"\n",
    "        NO_WORD_BEFORE = \"no_word_before\" \n",
    "        \n",
    "        ## For debugging, create a fixed-size buffer to hold the top_n words_before\n",
    "        ##import collections\n",
    "        ##debug_prev_words_deque = collections.deque(maxlen=top_n)\n",
    "        ##for i in range(top_n): debug_prev_words_deque.append(NO_WORD_BEFORE)\n",
    "        \n",
    "        word_list = [] # List of words used to determine the top_n\n",
    "        word_dict = {} # Dictionary of words used to hold their preceding words\n",
    "        token_before = \"\"\n",
    "        for doc in self.doc_list:\n",
    "            for token in doc:\n",
    "                token_no_end_punc = token.lower_.rstrip(string.punctuation)\n",
    "                \n",
    "                # Allow for matching multiple POS, e.g. \"NOUN,PROPN\" or all POS if pos=None\n",
    "                if not pos or token.pos_ in pos.split(\",\"):\n",
    "                    if token_no_end_punc not in exclude_words:\n",
    "                        word_list.append(token_no_end_punc)\n",
    "                        ##if token_no_end_punc == \"<some anomaly>\": print(\"DEBUG: '<some anomaly>' was preceded by: \", debug_prev_words_deque)\n",
    "\n",
    "                        if token_no_end_punc not in word_dict:\n",
    "                            word_dict[token_no_end_punc] = []\n",
    "\n",
    "                        if not token_before or token_before in string.punctuation:\n",
    "                            token_before = NO_WORD_BEFORE\n",
    "\n",
    "                        # Add the word before the specified part-of-speech word to the list of words found before it\n",
    "                        word_dict[token_no_end_punc].append(token_before)\n",
    "                        \n",
    "                token_before = token_no_end_punc\n",
    "                ##debug_prev_words_deque.append(token_before)\n",
    "        most_common_words = Counter(word_list).most_common(top_n)\n",
    "        if top_n:\n",
    "            print(\"Top\", top_n, end='')\n",
    "        else:\n",
    "            print(\"All\", end='')\n",
    "        print(pos + \"s and top preceeding words for\", self.name)\n",
    "        for word, word_count in most_common_words:\n",
    "            print(\"['\" + word + \"', \" + str(word_count) + \"]\", \"has top preceding words (freq>1): \", end=(''))\n",
    "            \n",
    "            most_common_words_before = Counter(word_dict[word]).most_common(top_n)\n",
    "            for word_before, word_before_count in most_common_words_before:\n",
    "                if word_before_count > 1:\n",
    "                    print(\"('\" + word_before + \"', \" + str(word_before_count) + \") \", end=(''))\n",
    "            print()\n",
    "        \n",
    "    def get_most_common_nouns(self, top_n = 10, exclude_words=[]):\n",
    "        return self.get_most_common_pos(\"NOUN\", top_n, exclude_words)\n",
    "\n",
    "    def print_most_common_nouns(self, top_n = 10, exclude_words=[]):\n",
    "        self.print_most_common_pos(\"NOUN\", top_n, exclude_words)\n",
    "\n",
    "    def get_most_common_nouns_and_propns(self, top_n = 10, exclude_words=[]):\n",
    "        return self.get_most_common_pos(\"NOUN,PROPN\", top_n, exclude_words)\n",
    "\n",
    "    def print_most_common_nouns_and_propns(self, top_n = 10, exclude_words=[]):\n",
    "        self.print_most_common_pos(\"NOUN,PROPN\", top_n, exclude_words)\n",
    "\n",
    "    def get_most_common_adjs(self, top_n = 10, exclude_words=[]):\n",
    "        return self.print_most_common_pos(\"ADJ\", top_n, exclude_words)\n",
    "        \n",
    "    def print_most_common_adjs(self, top_n = 10, exclude_words=[]):\n",
    "        self.print_most_common_pos(\"ADJ\", top_n, exclude_words)\n",
    "\n",
    "    def get_most_common_verbs(self, top_n = 10, exclude_words=[]):\n",
    "        return self.print_most_common_pos(\"VERB\", top_n, exclude_words)\n",
    "\n",
    "    def print_most_common_verbs(self, top_n = 10, exclude_words=[]):\n",
    "        self.print_most_common_pos(\"VERB\", top_n, exclude_words)\n",
    "        \n",
    "    def get_most_common_adverbs(self, top_n = 10, exclude_words=[]):\n",
    "        return self.print_most_common_pos(\"ADV\", top_n, exclude_words)\n",
    "        \n",
    "    def print_most_common_adverbs(self, top_n = 10, exclude_words=[]):\n",
    "        self.print_most_common_pos(\"ADV\", top_n, exclude_words)\n",
    "        \n",
    "    def print_most_common_noun_chunks(self, top_n = 10, exclude_list=[]):\n",
    "        \"\"\"\n",
    "        Print top_n noun chunks (noun phrases) - potentially giving more context to nouns;\n",
    "        specify 'None' for all.\n",
    "        ??? TODO: ** possibly make lower_ and rstrip configurable?\n",
    "        \"\"\"\n",
    "        noun_chunk_list = []\n",
    "        for doc in self.doc_list:\n",
    "            for chunk in doc.noun_chunks:\n",
    "                noun_chunk_str = ''\n",
    "                if exclude_list == None:\n",
    "                    noun_chunk_str = chunk.lower_.rstrip(string.punctuation)\n",
    "                else:\n",
    "                    for w in chunk.lower_.split():\n",
    "                        w_no_end_punc = w.rstrip(string.punctuation)\n",
    "                        if w_no_end_punc not in exclude_list:\n",
    "                            noun_chunk_str += w_no_end_punc + ' '\n",
    "                noun_chunk_str = noun_chunk_str.strip()\n",
    "                if len(noun_chunk_str) > 0:\n",
    "                    noun_chunk_list.append(noun_chunk_str)\n",
    "        most_common_noun_chunks = Counter(noun_chunk_list).most_common(top_n)\n",
    "        print(most_common_noun_chunks)\n",
    "        \n",
    "    def print_most_common_dep_trees(self, top_n = 10, exclude_list=[]):\n",
    "        \"\"\"\n",
    "        Print top_n sentence dependency trees - potentially giving more context to text.\n",
    "        top_n is the number of items to print; specify 'None' for all\n",
    "        TODO: tidy the output for more meaning.\n",
    "        \"\"\"\n",
    "\n",
    "        if exclude_list != None:\n",
    "            print(\"** exclude_list not implemented in order to preserve sentence structure\")\n",
    "        dep_tree_list = []\n",
    "        for doc in self.doc_list:\n",
    "            for token in doc:\n",
    "                if token.lower_ not in string.punctuation:\n",
    "                    dep_tree_str = \" \".join(\n",
    "                        [token.text, token.dep_, token.head.text, token.head.pos_, \"[\"])\n",
    "                    child_list = []\n",
    "                    for child in token.children:\n",
    "                        if child.lower_ not in string.punctuation:\n",
    "                            child_list.append(child.lower_.rstrip(string.punctuation))\n",
    "                    dep_tree_str += ','.join(child_list) + \"]\"\n",
    "                    dep_tree_list.append(dep_tree_str)\n",
    "        most_common_dep_trees = Counter(dep_tree_list).most_common(top_n)\n",
    "        if top_n:\n",
    "            print(\"Top\", top_n, end='')\n",
    "        else:\n",
    "            print(\"All\", end='')\n",
    "        print(\" dep trees for\", self.name, most_common_dep_trees)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download wordnet if not already done\n",
    "##nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find top word groups using synsets followed by spaCy similarity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name for group containings words that are not known in the language model (potentially spelling errors)\n",
    "UNKNOWN_GROUP_ROOT_WORD = \"_Unknown/Spelling_Error\"\n",
    "\n",
    "# Name for group containings words that are not known in the language model but were not grouped\n",
    "OTHER_GROUP_ROOT_WORD = \"_OTHER\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "## nltk.download('wordnet')\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus.reader.wordnet import WordNetError\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "def make_synset(word, category='n', number='01'):\n",
    "    \"\"\"Conveniently make a synset\"\"\"\n",
    "    number = int(number)\n",
    "    synset = wn.synset('%s.%s.%02i' % (word, category, number))\n",
    "    return synset\n",
    "    \n",
    "def _recurse_all_hypernyms(synset, all_hypernyms):\n",
    "    synset_hypernyms = synset.hypernyms()\n",
    "    if synset_hypernyms:\n",
    "        all_hypernyms += synset_hypernyms\n",
    "        for hypernym in synset_hypernyms:\n",
    "            _recurse_all_hypernyms(hypernym, all_hypernyms)\n",
    "\n",
    "def all_hypernyms(synset):\n",
    "    \"\"\"Get the set of hypernyms of the hypernym of the synset etc.\n",
    "       Nouns can have multiple hypernyms, so we can't just create a depth-sorted\n",
    "       list.\"\"\"\n",
    "    hypernyms = []\n",
    "    _recurse_all_hypernyms(synset, hypernyms)\n",
    "    return set(hypernyms)\n",
    "\n",
    "def _recurse_all_hyponyms(synset, all_hyponyms, max_depth=None):\n",
    "    synset_hyponyms = synset.hyponyms()\n",
    "    if synset_hyponyms:\n",
    "        all_hyponyms += synset_hyponyms\n",
    "        if max_depth:\n",
    "            for level, hyponym in enumerate(synset_hyponyms):\n",
    "                if (level < max_depth-1):\n",
    "                    _recurse_all_hyponyms(hyponym, all_hyponyms)\n",
    "                else: break\n",
    "        else:\n",
    "            for hyponym in synset_hyponyms:\n",
    "                _recurse_all_hyponyms(hyponym, all_hyponyms)\n",
    "            \n",
    "def all_hyponyms(synset, max_depth=None):\n",
    "    \"\"\"Get the set of the tree of hyponyms under the synset\"\"\"\n",
    "    hyponyms = []\n",
    "    _recurse_all_hyponyms(synset, hyponyms, max_depth)\n",
    "    return set(hyponyms)\n",
    "\n",
    "def all_peers(synset):\n",
    "    \"\"\"Get the set of all peers of the synset (including the synset).\n",
    "       If the synset has multiple hypernyms then the peers will be hyponyms of\n",
    "       multiple synsets.\"\"\"\n",
    "    hypernyms = synset.hypernyms()\n",
    "    peers = []\n",
    "    for hypernym in hypernyms:\n",
    "        peers += hypernym.hyponyms()\n",
    "    return set(peers)\n",
    "\n",
    "def synset_word(synset):\n",
    "    return synset.name().split('.')[0]\n",
    "\n",
    "def synsets_words(synsets):\n",
    "    \"\"\"Get the set of strings for the words represented by the synsets\"\"\"\n",
    "    return set([synset_word(synset) for synset in synsets])\n",
    "\n",
    "def get_hypernyms(word):\n",
    "    \"\"\"\n",
    "    Get hypernyms for the specified word.\n",
    "    \"\"\"\n",
    "    synset = make_synset(word)\n",
    "    if synset:\n",
    "        hypernyms = synsets_words(all_hypernyms(synset))\n",
    "    else:\n",
    "        hypernyms = None\n",
    "    return hypernyms\n",
    "\n",
    "def get_hyponyms_and_for_peers(word):\n",
    "    \"\"\"\n",
    "    Get hyponyms for the specified word and hyponyms of the word's peers.\n",
    "    Output appears much too general e.g. for such words as 'work'\n",
    "    \"\"\"\n",
    "    synset = make_synset(word)\n",
    "    if synset:\n",
    "        hyponyms = synsets_words(all_hyponyms(synset))\n",
    "        peers = synsets_words(all_peers(synset))\n",
    "        hyponyms_of_peers = set()\n",
    "        for s in all_peers(synset):\n",
    "            hyponyms_of_peers |= synsets_words(all_hyponyms(s))\n",
    "        hyponyms_and_for_peers = hyponyms | peers | hyponyms_of_peers\n",
    "    else:\n",
    "        hyponyms_and_for_peers = None\n",
    "    return hyponyms_and_for_peers\n",
    "\n",
    "def get_hyponyms(word, max_depth=None):\n",
    "    \"\"\"\n",
    "    Get hyponyms for the specified word.\n",
    "    Use max_depth to specify the level of hyponym to extract\n",
    "    \"\"\"\n",
    "    synset = make_synset(word)\n",
    "    if synset:\n",
    "        hyponyms = synsets_words(all_hyponyms(synset, max_depth))\n",
    "    else:\n",
    "        hyponyms = None\n",
    "    return hyponyms\n",
    "\n",
    "def get_top_word_groups_by_synset_then_similarity(\n",
    "    nlp, word_freqs, n_word_groups, max_hyponyms, max_hyponym_depth, sim_threshold, user_defined_groups):\n",
    "    \"\"\"\n",
    "    Function to print the top 'n' word \"synonym\" groups using WordNet synsets followed by\n",
    "    spaCy similarity:\n",
    "    - nlp: spaCy object pre-initialised with the required langauge model\n",
    "    - word_freqs: list of (word,count) tuples in (decreasing) order of frequency\n",
    "    - n_word_groups: number of word groups to find (specify 'None' means find all word/'synonym' groups)\n",
    "    - max_hyponyms: the maximum number of hyponyms a word may have before it is ignored (this is used to\n",
    "      exclude very general words that may not convey useful information)\n",
    "    - max_hyponym_depth specifies the level of hyponym to extract ('None' means find deepest)\n",
    "    - sim_threshold: the spaCy similarity level that words must reach to qualify as being a 'synonym'\n",
    "    - user_defined_groups: initial user-defined groupings. Note that \"synonyms\" for artificial words, such as\n",
    "      'love_it', that are not known in the language model (either WordNet or spaCy) can only be matched exactly (as\n",
    "      clearly no synonym can be found). An additional restriction, is that artifical words must be listed as root\n",
    "      words and may not have pre-set \"synonyms\". An alternative where the \"synonyms\" of each root word is searched \n",
    "      for equivalence (or even \"synonymity\") might be possible but this would come with performance cost.\n",
    "    Returns list of tuples:\n",
    "    - (root-word word frequency tuple, list of root and synonym word frequency tuples)\n",
    "    - (OTHER-word frequency tuple if any are not matched, list of other word frequency tuples)\n",
    "    - (UNKNOWN-word frequency tuple if any are not found in the language model, list of unknown word frequency tuples)\n",
    "\n",
    "    Some considerations:\n",
    "    - The use of WordNet synsets or spaCy similarity by themselves are as useful as required. This is for 2 reasons:\n",
    "      (1) Some of the Yale survey responses (both emotions and their reasons) contain both formal and informal\n",
    "      language. For example, shit'. The advantage of using spaCY with language model 'en_core_web_lg' is that this\n",
    "      model was trained on both formal and informal language. However, the word vectors in this spaCy language model\n",
    "      incorporate all senses of a word and each part of speech. \n",
    "    - Hence a combination of WordNet synsets and spaCy similarity is used. WordNet synsets are applied first to parse\n",
    "      formally defined words, then spaCy similarity is used to catch more informal words. A possible further\n",
    "      refinement might be to take user-defined groupings.\n",
    "    - It is assumed that 'stop' words and certain artificial words have been removed if required *before* the\n",
    "      function call (very common words such as \"I\" that may not be very useful in the word grouping)\n",
    "    - There are US-specific usages in the survey text, e.g. 'pissed' used as the emotion 'angry'.\n",
    "\n",
    "    Note: spaCy internally uses gloVe word vectors - see citations above.\n",
    "        \n",
    "    \"\"\"\n",
    "    # Word \"lemmatization\" is necessary as plurals are not stored in the WordNet synsets\n",
    "    Lem = WordNetLemmatizer()\n",
    "    \n",
    "    # Create a dictionary containing the spaCy token for each word\n",
    "    # - for later use with spaCy similarity\n",
    "    document = ''\n",
    "    for word_freq in word_freqs:\n",
    "        word, count = word_freq\n",
    "        document += word + ' '        \n",
    "    spacy_dict = {}\n",
    "    for spacy_token in nlp(document):\n",
    "        word = spacy_token.lower_\n",
    "        spacy_dict[word] = spacy_token\n",
    "    #for item in spacy_dict: print(spacy_dict[item])\n",
    "    \n",
    "    # Build a list of most-common word \"groups\", grouping words that are WordNet \"hyponyms\"\n",
    "    most_common = {}                   # The most common words and their synonyms\n",
    "    potential_spelling_errors = set()\n",
    "    already_grouped = set()            # Words already grouped\n",
    "    first_stored = False\n",
    "    \n",
    "    if user_defined_groups:\n",
    "        # Add the user-defined \"synonym\" groups\n",
    "        for _tuple in user_defined_groups:\n",
    "            root_word_tuple, root_and_syn_tuple_list = _tuple\n",
    "            root_word, root_and_syn_count = root_word_tuple\n",
    "            root_word = root_word.lower()\n",
    "            if root_word[0] == '_': root_word = root_word[1:]\n",
    "            print(\"DEBUG: Adding pre-defined root word '\" + root_word + \"' with root_and_syn_tuple_list:\",\n",
    "                  root_and_syn_tuple_list)\n",
    "            try:\n",
    "                lemword = Lem.lemmatize(root_word)\n",
    "                hyponyms = get_hyponyms(lemword, max_hyponym_depth)\n",
    "            except WordNetError:\n",
    "                # Word not known in the WordNet synsets but store it anyway as it's a pre-defined word\n",
    "                # but mark it as a potential spelling error\n",
    "                ##print(\"DEBUG: pre-defined word not known in the WordNet synsets: '\" + root_word + \"'\")\n",
    "                lemword = root_word\n",
    "                hyponyms = []\n",
    "                potential_spelling_errors.add(root_word)\n",
    "            most_common[root_word] = {}\n",
    "            most_common[root_word]['root_and_syn_count'] = 0\n",
    "            most_common[root_word]['root_and_syns'] = root_and_syn_tuple_list\n",
    "            most_common[root_word]['hyponyms'] = hyponyms\n",
    "            print(\"DEBUG: with hyponyms:\", hyponyms)\n",
    "            most_common[root_word]['lemmatization'] = lemword\n",
    "            print(\"DEBUG: and lemmatization:\", lemword)\n",
    "            try:\n",
    "                most_common[root_word]['root_token'] = spacy_dict[root_word]\n",
    "            except KeyError:\n",
    "                # No spaCy token found for the root token so create one\n",
    "                spaCy_doc_root_word = nlp(root_word)\n",
    "                spaCy_token_root_word = None\n",
    "                for item in spaCy_doc_root_word:\n",
    "                    spaCy_token_root_word = item                        \n",
    "                most_common[root_word]['root_token'] = spaCy_token_root_word\n",
    "            already_grouped.add(root_word)\n",
    "            first_stored = True\n",
    "            for word_tuple in root_and_syn_tuple_list:\n",
    "                word, word_count = word_tuple\n",
    "                print(\"DEBUG: with pre-defined synonym: '\" + word + \"'\")\n",
    "            \n",
    "    for word_freq in word_freqs:\n",
    "        word, count = word_freq\n",
    "        word = word.lower()\n",
    "        if word not in already_grouped:\n",
    "            ##print(\"DEBUG: word: '\" + word + \"', count: \" + str(count))\n",
    "            if (not first_stored):\n",
    "                try:\n",
    "                    lemword = Lem.lemmatize(word)\n",
    "                    hyponyms = get_hyponyms(lemword, max_hyponym_depth)\n",
    "                    if hyponyms and len(hyponyms) > max_hyponyms:\n",
    "                        continue\n",
    "                        ##print(\"DEBUG: Word '\" + word + \"'\" + \"(lemmatized as '\" + lemword + \"') exceeds hyponym limit\")\n",
    "                    else:\n",
    "                        # Store the first most common word as the first word group\n",
    "                        ##print(\"DEBUG: Adding root word '\" + word + \"' with hyponyms:\", hyponyms)\n",
    "                        most_common[word] = {}\n",
    "                        most_common[word]['root_and_syn_count'] = count\n",
    "                        most_common[word]['root_and_syns'] = [word_freq]\n",
    "                        most_common[word]['hyponyms'] = hyponyms\n",
    "                        most_common[word]['lemmatization'] = lemword\n",
    "                        most_common[word]['root_token'] = spacy_dict[word]\n",
    "                        already_grouped.add(word)\n",
    "                        first_stored = True\n",
    "                except WordNetError:\n",
    "                    ##print(\"DEBUG: Word not known in the WordNet synsets: '\" + word + \"'\")\n",
    "                    potential_spelling_errors.add(word)\n",
    "            else:\n",
    "                # Store the common word as a new word group if there are no existing\n",
    "                # \"synonyms\" in the most_common dictionary\n",
    "                synonym_or_match_found = False\n",
    "                for common in most_common:\n",
    "                    if common == word:\n",
    "                        # word is already a 'root' word group\n",
    "                        synonym_or_match_found = True\n",
    "                    else:\n",
    "                        if common not in potential_spelling_errors:\n",
    "                            if word not in already_grouped:\n",
    "                                lemword = Lem.lemmatize(word)\n",
    "                                if (lemword == most_common[common]['lemmatization'] or\n",
    "                                    lemword in most_common[common]['hyponyms']):\n",
    "                                    # synonym found - incorporate it under the 'root' synonym\n",
    "                                    synonym_or_match_found = True\n",
    "                                    most_common[common]['root_and_syn_count'] += count\n",
    "                                    most_common[common]['root_and_syns'].append(word_freq)\n",
    "                                    already_grouped.add(word)\n",
    "\n",
    "                if not synonym_or_match_found:\n",
    "                    try:\n",
    "                        lemword = Lem.lemmatize(word)\n",
    "                        hyponyms = get_hyponyms(lemword, max_hyponym_depth)\n",
    "                        if hyponyms and len(hyponyms) > max_hyponyms:\n",
    "                            continue\n",
    "                            ##print(\"DEBUG: Word '\" + word + \"'\" + \"(lemmatized as '\" + lemword + \"') exceeds hyponym limit\")\n",
    "                        else:\n",
    "                            ##print(\"DEBUG: Adding root word '\" + word + \"' with hyponyms:\", hyponyms)\n",
    "                            most_common[word] = {}\n",
    "                            most_common[word]['root_and_syn_count'] = count\n",
    "                            most_common[word]['root_and_syns'] = [word_freq]\n",
    "                            most_common[word]['hyponyms'] = hyponyms\n",
    "                            most_common[word]['lemmatization'] = lemword\n",
    "                            most_common[word]['root_token'] = spacy_dict[word]\n",
    "                            already_grouped.add(word)\n",
    "                    except WordNetError:\n",
    "                        ##print(\"DEBUG: Word not known in the WordNet synsets: '\" + word + \"'\")\n",
    "                        potential_spelling_errors.add(word)\n",
    "\n",
    "    # Create a spaCy token for a common word such as 'the' - for later use in detecting unknown words\n",
    "    spaCy_doc_the = nlp(\"the\")\n",
    "    spaCy_token_the = None\n",
    "    for item in spaCy_doc_the:\n",
    "        spaCy_token_the = item                        \n",
    "\n",
    "    # Now apply spaCy similarity to try to group words for which a hyponym was not found or\n",
    "    # which were not known in the WordNet synsets ...\n",
    "    other_words = set()\n",
    "    for word_freq in word_freqs:\n",
    "        word, count = word_freq\n",
    "        if word not in already_grouped: other_words.add(word)\n",
    "        ##print(\"DEBUG: Adding\", word, \"not in already_grouped to other_words\")\n",
    "    for word in potential_spelling_errors:\n",
    "        other_words.add(word)\n",
    "        ##print(\"DEBUG: Adding\", word, \"in potential_spelling_errors to other_words\")\n",
    "\n",
    "    # ... i.e. update the list of most-common word \"groups\" formed from WordNet synsets with words that have similar\n",
    "    # enough spaCy token.similarity. Words that were not known in synsets may be known in spaCy so reset the list of\n",
    "    # potential spelling errors. Note tokens with a length of 1 are ignored as a work-around for a spaCy\n",
    "    # token.similarity(other) bug\n",
    "\n",
    "    potential_spelling_errors = set()\n",
    "    already_grouped = set()\n",
    "    first_stored = False\n",
    "    for word_freq in word_freqs:\n",
    "        word, count = word_freq\n",
    "        if word in other_words:\n",
    "            ##print(\"DEBUG: word in other_words: '\" + word + \"', count: \" + str(count))\n",
    "            if word not in already_grouped:\n",
    "                ##print(\"DEBUG: word not in already_grouped: '\" + word + \"', count: \" + str(count))\n",
    "                if len(word) > 1: # Work-around for bug in spaCy if token has length of 1\n",
    "                    synonym_or_match_found = False\n",
    "                    best_match = None\n",
    "                    best_match_similarity = 0\n",
    "                    for common in most_common:\n",
    "                        if len(common) > 1:    # Work-around for bug in spaCy if token has length of 1\n",
    "                            common_token = most_common[common]['root_token']\n",
    "                            try:\n",
    "                                token = spacy_dict[word]\n",
    "                            except KeyError:\n",
    "                                # No spaCy token found for the word so create one\n",
    "                                spaCy_doc_word = nlp(word)\n",
    "                                spaCy_token_word = None\n",
    "                                for item in spaCy_doc_word:\n",
    "                                    spaCy_token_word = item                        \n",
    "                                    token = spaCy_token_word\n",
    "                            ##print(\"DEBUG: Similarity between token '\", token, \"' and common item '\",\n",
    "                            ##      common, \"' is:\", token.similarity(common_token))\n",
    "                            if (common_token.lower_ == token.lower_):\n",
    "                                # word is already a 'root' word group\n",
    "                                synonym_or_match_found = True\n",
    "                                break\n",
    "                            else:\n",
    "                                # check \"word\" is not a potential spelling error/unknown to spaCy\n",
    "                                if common_token.lower_ not in potential_spelling_errors:\n",
    "                                    similarity = token.similarity(common_token)\n",
    "                                    if similarity >= sim_threshold:\n",
    "                                        if similarity > best_match_similarity:\n",
    "                                            best_match = common\n",
    "                                            best_match_similarity = similarity\n",
    "                                    elif similarity == 0:\n",
    "                                        if word not in potential_spelling_errors:\n",
    "                                            # Check similarity of word against a known word such as'the' in case\n",
    "                                            # common_token is a word that is known by WordNet but not by spaCy\n",
    "                                            if token.similarity(spaCy_token_the) == 0:\n",
    "                                                ##print(\"DEBUG: Adding potential spelling error for '\" + word,\n",
    "                                                ##      \"' - common_token was '\" + common_token.lower_ +\n",
    "                                                ##      \"' similarity was \" + str(similarity))\n",
    "                                                potential_spelling_errors.add(word)\n",
    "                                                break\n",
    "                    if best_match:\n",
    "                        # incorporate the word under the 'root' synonym with the best similarity match\n",
    "                        synonym_or_match_found = True\n",
    "                        most_common[best_match]['root_and_syn_count'] += count\n",
    "                        most_common[best_match]['root_and_syns'].append(word_freq)\n",
    "                        already_grouped.add(word)\n",
    "                        \n",
    "    top_word_groups = []\n",
    "    other_words = []\n",
    "    for index, word in enumerate(most_common):\n",
    "        if index < n_word_groups:\n",
    "            word_group_tuple = (\"_\" + word, most_common[word]['root_and_syn_count'])\n",
    "            root_and_syns_list = []\n",
    "            for word_freq in most_common[word]['root_and_syns']:\n",
    "                root_and_syns_list.append(word_freq)\n",
    "            top_word_groups.append((word_group_tuple, root_and_syns_list))\n",
    "        elif word not in potential_spelling_errors:\n",
    "            other_words.append(word)\n",
    "            \n",
    "    if other_words:\n",
    "        # Words that were not potential spelling errors that did not make it into a word group\n",
    "        total_other_words = 0\n",
    "        other_word_freqs = []\n",
    "        for word in other_words:\n",
    "            word_count = 0\n",
    "            for word_freq in word_freqs:\n",
    "                w, count = word_freq\n",
    "                if w == word:\n",
    "                    word_count = count\n",
    "                    other_word_freqs.append(word_freq)\n",
    "                    break;\n",
    "            total_other_words += word_count\n",
    "        other_word_group_tuple = (OTHER_GROUP_ROOT_WORD, total_other_words)\n",
    "        top_word_groups.append((other_word_group_tuple, other_word_freqs))\n",
    "        \n",
    "    if len(potential_spelling_errors) > 0:\n",
    "        # Potential spelling errors / words not in the spaCy model that (may) have been ignored\n",
    "        total_unknown_words = 0\n",
    "        unknown_word_freqs = []\n",
    "        for word in potential_spelling_errors:\n",
    "            word_count = 0\n",
    "            for word_freq in word_freqs:\n",
    "                w, count = word_freq\n",
    "                if w == word:\n",
    "                    word_count = count\n",
    "                    unknown_word_freqs.append(word_freq)\n",
    "                    break;\n",
    "            total_unknown_words += word_count\n",
    "        unknown_word_group_tuple = (UNKNOWN_GROUP_ROOT_WORD, total_unknown_words)\n",
    "        top_word_groups.append((unknown_word_group_tuple, unknown_word_freqs))\n",
    "    \n",
    "    # Check all words were accounted for\n",
    "    for word in already_grouped:\n",
    "        word_found = False\n",
    "        for word_group in most_common:\n",
    "            for word_freq in most_common[word_group]['root_and_syns']:\n",
    "                w, count = word_freq\n",
    "                if word == w: word_found = True\n",
    "        if not word_found: print(\"*** Error: \" + word + \" not found in most_common\")\n",
    "    \n",
    "    return top_word_groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print topics by finding words associated with each noun group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def get_words_around(target, text, n_words):\n",
    "    \"\"\"\n",
    "    Helper function for print_words_associated_with_common_noun_groups to return n_words before or\n",
    "    after each occurence of a word in some text.\n",
    "    The function returns a list (or lists) of the n_words before and after the 'target' word.\n",
    "    \"\"\"\n",
    "    words = text.lower().rstrip(string.punctuation).split()\n",
    "    for i,w in enumerate(words):\n",
    "        if w == target:\n",
    "            if (i<n_words): yield words[0:i], words[i+1:i+1+n_words]\n",
    "            else: yield words[i-n_words:i], words[i+1:i+1+n_words]\n",
    "    \n",
    "def print_words_associated_with_common_noun_groups(\n",
    "    nlp, name, free_text_Series, exclude_words, top_n_noun_groups, top_n_words,\n",
    "    max_hyponyms, max_hyponym_depth, sim_threshold):\n",
    "    \"\"\"\n",
    "    Find the top noun groups (nouns with 'synonyms') in free_text_Series and print the top_n_words associated\n",
    "    with them:\n",
    "    - nlp: spaCy object pre-initialised with the required langauge model\n",
    "    - name: descriptive name for free_text_Series\n",
    "    - free_text_Series: pandas Series of free_text in which to find the noun groups and associated words\n",
    "    - exclude_words: to ignore certain words, e.g. not so useful 'stop words' or artificial words\n",
    "    - top_n_noun_groups: number of noun groups to find (specify 'None' means find all noun/'synonym' groups)\n",
    "    - top_n_words: number of words that are associated with each noun group (specify 'None' for all words)\n",
    "    - max_hyponyms: the maximum number of hyponyms a word may have before it is ignored (this is used to\n",
    "      exclude very general words that may not convey useful information)\n",
    "    - max_hyponym_depth: the the level of hyponym to extract ('None' means find deepest)\n",
    "    - sim_threshold: the spaCy similarity level that words must reach to qualify as being a 'synonym'\n",
    "    Notes:\n",
    "    (1) For best results, stop words should be excluded\n",
    "    (2) If stop words are excluded, negations such as \"no\", \"not\" and words ending with \"n't\" are still considered\n",
    "        in the parsing (within the limits of the word context length NUM_CONTEXT_WORDS)\n",
    "    (3) If no associated word is found, it is assumed the word itself is the only context for the text. For example,\n",
    "        with noun group \"_work\", the following free text items would result in the noun itself ('work') being\n",
    "        reported as an associated word: 'The work that I do', 'Work', 'work.'\n",
    "        This seems to work in the majority of sample of cases used but has not been exhaustively tested. In\n",
    "        particular, this may give surprising results if the \"synonym\" matching does not actually give the desired\n",
    "        synonyms and incorrect results if NUM_CONTEXT_WORDS is too short to capture significant context.\n",
    "        \n",
    "    Known restrictions/issues:\n",
    "    (a) The parsing of associated words looks only at words that are up to NUM_CONTEXT_WORDS words before or after\n",
    "        each noun, inclusive of any exclude words. This limit could be parameterised in this function (it already is\n",
    "        in helper function 'get_words_around')\n",
    "    (b) The context words before the noun are parsed first since in the samples tested it appears the \"before-words\"\n",
    "        usually give more context than words proceeding a nound. Clearly. this may give sub-optimal results for text\n",
    "        where the main context for the noun comes in the words following it.\n",
    "    (c) The parsing of hyphenated words may not always work correctly.\n",
    "    (d) Spelling mistakes or grammatical errors in the original text may give surprising results.\n",
    "    (e) See also notes in helper function get_top_word_groups_by_synset_then_similarity and class SpaCyFreeText\n",
    "    (f) Significantly more testing is required to verify usefulness and robusteness.\n",
    "\n",
    "    Possible enhancements:\n",
    "    (i)   NUM_CONTEXT_WORDS could be parameterised, as described above\n",
    "    (ii)  The output of associated words could be limited to those having a frequency>1\n",
    "    (iii) Words associated with each noun could be additionally grouped in \"valence\" sub-groups\n",
    "    (iv)  For large text, parallelisation would help performance.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Parsing\", name, \"using English language model\", SPACY_MODEL)\n",
    "    text_spaCy = SpaCyFreeText(nlp, name, free_text_Series)\n",
    "    free_text_list = text_spaCy.get_free_text_list()\n",
    "    \n",
    "    # Get frequencies of all nouns and proper nouns in the free-text Series\n",
    "    all_noun_and_propn_freqs = text_spaCy.get_most_common_nouns_and_propns(None, exclude_words)\n",
    "    print(str(len(all_noun_and_propn_freqs)), \"unique nouns/proper nouns found\")\n",
    "    \n",
    "    # Get \"synonym\" groupings of the nouns and proper nouns (either wordnet hyponyms or spaCy similarity)\"\n",
    "    top_word_groups = get_top_word_groups_by_synset_then_similarity(\n",
    "        nlp, all_noun_and_propn_freqs, top_n_noun_groups, max_hyponyms, max_hyponym_depth, sim_threshold, None)\n",
    "    \n",
    "    NUM_CONTEXT_WORDS = 4\n",
    "    if top_n_words:\n",
    "        print(\"Top\", top_n_words, \"words\", end='')\n",
    "    else:\n",
    "        print(\"Words\", end='')\n",
    "    print(\" associated with nouns/proper noun groupings, looking at up to\",\n",
    "          str(NUM_CONTEXT_WORDS), \"words before or after each noun:\")\n",
    "\n",
    "    ##print(\"DEBUG:\", top_word_groups)\n",
    "    for item in top_word_groups:\n",
    "        root_word_freq, group_word_freqs = item\n",
    "        root_word, root_word_count = root_word_freq\n",
    "        if root_word != OTHER_GROUP_ROOT_WORD and root_word != UNKNOWN_GROUP_ROOT_WORD:\n",
    "            NO_WORD = \"no_word\"\n",
    "            assoc_word_list = [] # List of associated words for each noun/proper nount group\n",
    "            for word_freq in group_word_freqs:\n",
    "                word, count = word_freq\n",
    "                for free_text in free_text_list:\n",
    "                    if word in free_text.lower().rstrip(string.punctuation).split():\n",
    "                        ##print(\"DEBUG:--------------------------------------------------------------------------------\")\n",
    "                        ##print(\"DEBUG: free_text: '\" + free_text + \"'\")\n",
    "                        for before_words, after_words in get_words_around(word, free_text, NUM_CONTEXT_WORDS):\n",
    "                            ##print(\"DEBUG: word(s) before '\" + word + \"': \", before_words)\n",
    "                            ##print(\"DEBUG: word(s) after '\" + word + \"': \", after_words)\n",
    "                            # First try to find an associated word in words before the noun word\n",
    "                            assoc_word = None\n",
    "                            before_words_len = len(before_words)\n",
    "                            for index in range(before_words_len):\n",
    "                                before_word = before_words[before_words_len-index-1]\n",
    "                                if not before_word or before_word in string.punctuation: break\n",
    "                                elif before_word == \"not\" or before_word[-3:] == \"n't\":\n",
    "                                    negation = \"not\"\n",
    "                                    if index != 0 and before_words[before_words_len-index] != word:\n",
    "                                        negation = negation + \"_\" + before_words[before_words_len-index]\n",
    "                                    assoc_word = negation\n",
    "                                    ##print(\"DEBUG: negation '\" + negation + \"' added to assoc_word_list for word '\" +\n",
    "                                    ##      word + \"' in free_text '\" + free_text + \"'\")\n",
    "                                    break\n",
    "                                elif before_word == \"no\":\n",
    "                                    negation = \"no\"\n",
    "                                    if index != 0 and before_words[before_words_len-index] != word:\n",
    "                                        negation = negation + \"_\" + before_words[before_words_len-index]\n",
    "                                    assoc_word = negation\n",
    "                                    ##print(\"DEBUG: negation '\" + negation + \"' added to assoc_word_list for word '\" +\n",
    "                                    ##      word + \"' in free_text '\" + free_text + \"'\")\n",
    "                                    break\n",
    "                                elif before_word not in exclude_words:\n",
    "                                    word_before_before_word = before_words[before_words_len-index-2]\n",
    "                                    if word_before_before_word:\n",
    "                                        if word_before_before_word == \"not\" or word_before_before_word[-3:] == \"n't\":\n",
    "                                            negation = \"not_\" + before_word\n",
    "                                            assoc_word = negation\n",
    "                                            ##print(\"DEBUG: negation of before_word '\" + negation + \"' added to assoc_word_list for word '\" +\n",
    "                                            ##      word + \"' in free_text '\" + free_text + \"'\")\n",
    "                                            break\n",
    "                                        else:\n",
    "                                            assoc_word = before_word\n",
    "                                            ##print(\"DEBUG: before_word '\" + before_word + \"' added to assoc_word_list for word '\" +\n",
    "                                            ##      word + \"' in free_text '\" + free_text + \"'\")\n",
    "                                            break\n",
    "                            if not assoc_word:\n",
    "                                # Try to find an associated word following the noun word\n",
    "                                after_words_len = len(after_words)\n",
    "                                for index in range(after_words_len):\n",
    "                                    after_word = after_words[index]\n",
    "                                    if not after_word or after_word in string.punctuation: break\n",
    "                                    elif after_word == \"not\" or after_word[-3:] == \"n't\":\n",
    "                                        if (index+1 < after_words_len):\n",
    "                                            word_after_after_word = after_words[index+1]\n",
    "                                            negation = \"not_\" + word_after_after_word\n",
    "                                            assoc_word = negation\n",
    "                                            ##print(\"DEBUG: negation '\" + negation + \"' added to assoc_word_list for word '\" +\n",
    "                                            ##      word + \"' in free_text '\" + free_text + \"'\")\n",
    "                                        break\n",
    "                                    elif after_word == \"no\":\n",
    "                                        if (index+1 < after_words_len):\n",
    "                                            word_after_after_word = after_words[index+1]\n",
    "                                            negation = \"no_\" + word_after_after_word\n",
    "                                            assoc_word = negation\n",
    "                                            ##print(\"DEBUG: negation '\" + negation + \"' added to assoc_word_list for word '\" +\n",
    "                                            ##      word + \"' in free_text '\" + free_text + \"'\")\n",
    "                                        break\n",
    "                                    elif after_word not in exclude_words:\n",
    "                                        assoc_word_list.append(after_word)\n",
    "                                        assoc_word = after_word\n",
    "                                        ##print(\"DEBUG: after_word '\" + after_word + \"' added to assoc_word_list for word '\" +\n",
    "                                        ##      word + \"' in free_text '\" + free_text + \"'\")\n",
    "                                        break\n",
    "                        if not assoc_word:\n",
    "                            # Assume the word itself is the only context for the text - see Notes above\n",
    "                            assoc_word = word\n",
    "                            ##if word == root_word:\n",
    "                            ##    print(\"DEBUG: Topic identified: \" + root_word + \" for '\" + free_text + \"'\")\n",
    "                            ##else:\n",
    "                            ##    print(\"DEBUG: Topic identified: \" + root_word + \": \" + word + \" for '\" + \n",
    "                            ##          free_text + \"'\")\n",
    "                        ##else:\n",
    "                        ##    print(\"DEBUG: Topic identified: \" + root_word + \": \" + assoc_word + \"_\" + word\n",
    "                        ##          + \" for '\" + free_text + \"'\")\n",
    "                        assoc_word_list.append(assoc_word)\n",
    "                        \n",
    "            most_common_words = Counter(assoc_word_list).most_common(top_n_words)\n",
    "            ##print(\"DEBUG: assoc_word_list:\", assoc_word_list)\n",
    "            print(\"'\" + root_word + \"', \" + str(root_word_count) + \": \", end='')\n",
    "            print(most_common_words, end='')\n",
    "            print(\"    {\", end='')\n",
    "            for word_freq in group_word_freqs:\n",
    "                word, count = word_freq\n",
    "                print(\"('\" + word + \"', \" + str(count) + \"), \", end='')\n",
    "            print(\"}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing **Sample** Yale survey emotion reasons (FEELATWORK_TASKS_<n>_WHY_clean's) using English language model en_core_web_lg\n",
      "160 unique nouns/proper nouns found\n",
      "Words associated with nouns/proper noun groupings, looking at up to 4 words before or after each noun:\n",
      "'_job', 25: [('well', 4), ('like', 3), ('enjoy', 2), ('new', 2), ('good', 2), ('simple', 2), ('satisfying', 2), ('knowing', 1), ('finally', 1), ('not_secured', 1), ('maintain', 1), ('got', 1), ('many', 1), ('3', 1), ('lack', 1), ('career', 1), ('place', 1)]    {('job', 20), ('jobs', 3), ('career', 1), ('employment', 1), }\n",
      "'_work', 21: [('good', 2), ('hard', 2), ('retail', 2), ('3', 2), ('lot', 2), ('slow', 2), ('awesome', 2), ('well', 2), ('love', 1), ('much', 1), ('superb', 1), ('making', 1), ('work', 1), ('repetitive', 1), ('find', 1), ('place', 1), ('excited', 1), ('level', 1), ('completed', 1), (\"lot's\", 1), ('take', 1), ('deliver', 1), ('weight/shirk', 1), ('incomplete', 1), ('not', 1), ('things', 1), ('needs', 1), ('job', 1)]    {('work', 16), ('care', 1), ('services', 1), ('duties', 1), ('paperwork', 1), ('done', 1), }\n",
      "'_people', 19: [('helping', 5), ('irresponsible', 2), ('complaining', 2), ('situations', 2), ('use', 2), ('annoying', 1), ('not_get', 1), ('accountability', 1), ('dumb', 1), ('different', 1), ('help', 1), ('lot', 1), ('awesome', 1), ('clientele', 1), ('provides', 1)]    {('people', 15), ('clientele', 1), ('living', 1), ('others', 2), }\n",
      "'_time', 16: [('limits', 2), ('like', 2), ('little', 1), ('play', 1), ('idle', 1), ('take', 1), ('get', 1), ('not_know', 1), ('everything', 1), ('work', 1), ('rewarding', 1), ('multiple', 1)]    {('time', 13), ('times', 3), }\n",
      "'_management', 9: [('poor', 4), ('upper', 2), ('management', 1), ('engine', 1), ('micro', 1)]    {('management', 8), ('mismanagement', 1), }\n",
      "'_hours', 7: [('full-time', 2), ('worked', 2), ('many', 1), ('long', 1), ('70+', 1), ('late', 1)]    {('hours', 7), }\n",
      "'_day', 6: [('demands', 1), ('night', 1), ('jobs', 1), ('every', 1), ('kids', 1), ('done', 1)]    {('day', 6), }\n",
      "'_lot', 9: [('students', 2), ('going', 2), ('responsibility', 2), ('overtime', 2), ('accomplishing', 1), ('work', 1), ('meet', 1), ('lots', 1)]    {('lot', 6), ('lots', 3), }\n",
      "'_things', 8: [('went', 4), ('go', 2), ('every', 2), ('frustrated', 1), ('new', 1), ('get', 1), ('complete', 1)]    {('things', 6), ('thing', 1), ('everything', 1), }\n",
      "'_business', 6: [('growing', 2), ('slow', 2), ('well', 2), ('business', 1), ('built', 1), ('local', 1)]    {('business', 6), }\n"
     ]
    }
   ],
   "source": [
    "print_words_associated_with_common_noun_groups(\n",
    "    nlp, \"**Sample** Yale survey emotion reasons (FEELATWORK_TASKS_<n>_WHY_clean's)\",\n",
    "    sample_reasons['sample_reasons_concat'], exclude_words, 10, None, 100, 1, 0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed 29 Aug 11:28:06 BST 2018\r\n"
     ]
    }
   ],
   "source": [
    "!date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing Yale survey emotion reasons (FEELATWORK_TASKS_<n>_WHY_clean's) using English language model en_core_web_lg\n",
      "3238 unique nouns/proper nouns found\n",
      "Top 10 words associated with nouns/proper noun groupings, looking at up to 4 words before or after each noun:\n",
      "'_job', 4212: [('like', 412), ('love', 331), ('good', 286), ('job', 283), ('well', 162), ('enjoy', 136), ('new', 124), ('great', 80), ('not_doing', 71), ('security', 68)]    {('job', 3716), ('jobs', 211), ('position', 138), ('career', 64), ('employment', 22), ('positions', 18), ('profession', 9), ('game', 5), ('appointments', 5), ('catering', 4), ('trade', 4), ('careers', 4), ('games', 3), ('sports', 3), ('trades', 2), ('employments', 1), ('sport', 1), ('photography', 1), ('appointment', 1), }\n",
      "'_work', 3989: [('much', 348), ('work', 332), ('hard', 265), ('people', 245), ('lot', 233), ('load', 159), ('good', 158), ('job', 128), ('get', 119), ('well', 119)]    {('work', 3485), ('care', 103), ('service', 89), ('duties', 78), ('paperwork', 78), ('labor', 39), ('actions', 29), ('services', 25), ('mission', 12), ('action', 10), ('works', 8), ('duty', 7), ('operation', 6), ('operations', 4), ('busywork', 1), ('done', 15), }\n",
      "'_people', 2800: [('helping', 307), ('help', 269), ('work', 239), ('people', 129), ('good', 67), ('like', 63), ('make', 48), ('new', 45), ('many', 41), ('not_doing', 39)]    {('people', 2192), ('living', 21), ('peoples', 21), ('class', 18), ('clientele', 6), ('folks', 5), ('population', 3), ('blood', 3), ('classes', 3), ('dead', 1), ('nation', 1), ('others', 512), ('individuals', 14), }\n",
      "'_time', 1125: [('not_enough', 116), ('little', 50), ('time', 38), ('long', 34), ('done', 24), ('amount', 23), ('lot', 23), ('busy', 22), ('work', 22), ('full', 21)]    {('time', 977), ('times', 148), }\n",
      "'_hours', 999: [('long', 321), ('many', 113), ('hours', 41), ('work', 37), ('not_enough', 33), ('lot', 33), ('cut', 21), ('worked', 21), ('extra', 15), ('12', 15)]    {('hours', 943), ('hour', 56), }\n",
      "'_lot', 1135: [('work', 418), ('things', 54), ('responsibility', 52), ('lot', 51), ('responsibilities', 44), ('going', 32), ('hours', 30), ('time', 29), ('not_a', 28), ('get', 27)]    {('lot', 827), ('lots', 308), }\n",
      "'_things', 1026: [('new', 93), ('get', 87), ('going', 58), ('every', 34), ('everyday', 34), ('many', 32), ('go', 28), ('went', 28), ('getting', 26), ('thing', 26)]    {('things', 724), ('thing', 141), ('everything', 161), }\n",
      "'_customers', 909: [('customers', 119), ('rude', 77), ('help', 42), ('helping', 27), ('happy', 27), ('service', 24), ('angry', 18), ('nice', 17), ('satisfied', 17), ('dealing', 15)]    {('customers', 643), ('customer', 202), ('guests', 39), ('guest', 13), ('patrons', 6), ('patron', 2), ('policyholders', 2), ('shoppers', 1), ('buyers', 1), }\n",
      "'_day', 857: [('every', 97), ('long', 82), ('day', 46), ('good', 39), ('work', 36), ('slow', 21), ('end', 20), ('busy', 17), ('bad', 16), ('feet', 15)]    {('day', 629), ('days', 203), ('today', 12), ('date', 9), ('dates', 3), ('eve', 1), }\n",
      "'_tasks', 829: [('complete', 57), ('completed', 57), ('many', 52), ('new', 44), ('repetitive', 39), ('accomplished', 26), ('tasks', 18), ('completing', 18), ('difficult', 15), ('accomplish', 14)]    {('tasks', 556), ('task', 217), ('assignments', 28), ('assignment', 10), ('baby', 7), ('babies', 5), ('adventures', 3), ('adventure', 2), ('ventures', 1), }\n",
      "'_boss', 549: [('boss', 114), ('good', 15), ('great', 14), ('new', 14), ('bad', 13), ('idiot', 13), ('likes', 10), ('terrible', 9), ('never', 8), ('makes', 8)]    {('boss', 549), }\n",
      "'_money', 519: [('money', 104), ('making', 58), ('make', 45), ('enough', 45), ('good', 36), ('not_enough', 26), ('made', 16), ('need', 15), ('lot', 12), ('no', 12)]    {('money', 517), ('funds', 2), }\n",
      "'_employees', 668: [('employees', 58), ('new', 33), ('great', 18), ('not_enough', 18), ('fellow', 15), ('work', 15), ('not_doing', 14), ('good', 13), ('lack', 10), ('not_care', 9)]    {('employees', 507), ('employee', 138), ('hire', 8), ('hires', 5), ('clerk', 4), ('dispatcher', 4), ('spotter', 1), ('dispatchers', 1), }\n",
      "'_lack', 510: [('communication', 109), ('support', 74), ('sleep', 52), ('work', 50), ('training', 34), ('respect', 30), ('help', 25), ('direction', 20), ('resources', 12), ('staff', 12)]    {('lack', 501), ('absence', 5), ('absences', 3), ('deficit', 1), }\n",
      "'_management', 604: [('management', 82), ('upper', 62), ('poor', 59), ('bad', 23), ('support', 12), ('new', 11), ('lack', 10), ('makes', 10), ('unfair', 8), ('making', 7)]    {('management', 486), ('administration', 45), ('treatment', 31), ('supervision', 15), ('finances', 10), ('guidance', 6), ('managements', 4), ('mismanagement', 3), ('treatments', 2), ('finance', 1), ('managing', 1), }\n",
      "'_students', 459: [('students', 27), ('learning', 17), ('behavior', 17), ('make', 14), ('success', 14), ('seeing', 11), ('well', 11), ('helping', 10), ('love', 9), ('work', 8)]    {('students', 374), ('student', 81), ('major', 3), ('auditors', 1), }\n",
      "'_company', 378: [('good', 22), ('company', 12), ('great', 10), ('work', 10), ('well', 9), ('growing', 8), ('going', 8), ('within', 7), ('new', 7), ('makes', 6)]    {('company', 365), ('companies', 13), }\n",
      "'_pay', 330: [('low', 65), ('good', 63), ('pay', 53), ('raise', 28), ('not_enough', 22), ('day', 20), ('bills', 18), ('not', 16), ('little', 12), ('well', 11)]    {('pay', 327), ('pays', 1), ('paying', 2), }\n",
      "'_environment', 372: [('work', 97), ('environment', 32), ('go', 31), ('good', 25), ('great', 14), ('working', 14), ('hostile', 6), ('paced', 6), ('depot', 6), ('new', 5)]    {('environment', 278), ('home', 79), ('homes', 7), ('setting', 4), ('street', 2), ('environments', 1), ('settings', 1), }\n",
      "'_deadlines', 338: [('deadlines', 99), ('meet', 39), ('tight', 21), ('met', 18), ('deadline', 16), ('meeting', 14), ('many', 7), ('time', 7), ('approaching', 6), ('lot', 6)]    {('deadlines', 274), ('deadline', 64), }\n",
      "'_kids', 273: [('work', 41), ('kids', 21), ('working', 21), ('make', 12), ('helping', 11), ('lot', 8), ('love', 8), ('learning', 8), ('not_listening', 6), ('seeing', 6)]    {('kids', 262), ('kid', 8), ('preschoolers', 1), ('toddler', 1), ('peanuts', 1), }\n",
      "'_manager', 349: [('manager', 41), ('managers', 12), ('new', 9), ('always', 8), ('rude', 8), ('bad', 7), ('actions', 7), ('made', 7), ('idiot', 6), ('department', 5)]    {('manager', 239), ('managers', 110), }\n",
      "'_goals', 334: [('met', 34), ('sales', 19), ('achieved', 18), ('accomplished', 15), ('goals', 13), ('meet', 11), ('meeting', 10), ('reach', 10), ('achieve', 9), ('reaching', 8)]    {('goals', 238), ('goal', 79), ('purpose', 17), }\n",
      "'_clients', 355: [('clients', 26), ('happy', 13), ('new', 12), ('helping', 10), ('client', 10), ('help', 9), ('demanding', 6), ('deal', 6), ('love', 5), ('make', 5)]    {('clients', 230), ('client', 125), }\n",
      "'_project', 391: [('completed', 56), ('new', 42), ('complete', 22), ('many', 19), ('finished', 18), ('big', 16), ('finish', 9), ('going', 8), ('difficult', 7), ('finishing', 7)]    {('project', 228), ('projects', 163), }\n",
      "'_team', 248: [('great', 35), ('work', 19), ('good', 17), ('members', 15), ('part', 9), ('works', 7), ('management', 5), ('team', 5), ('new', 4), ('working', 4)]    {('team', 225), ('crew', 16), ('teams', 4), ('section', 2), ('sections', 1), }\n",
      "'_way', 273: [('feel', 24), ('things', 12), ('new', 10), ('felt', 8), ('no', 8), ('job', 8), ('management', 7), ('treated', 7), ('good', 7), ('much', 6)]    {('way', 221), ('ways', 29), ('response', 8), ('responses', 4), ('fit', 3), ('forms', 3), ('touch', 2), ('form', 2), ('fits', 1), }\n",
      "'_help', 430: [('people', 211), ('able', 90), ('no', 74), ('others', 68), ('get', 65), ('lack', 57), ('not_enough', 55), ('not', 23), ('need', 22), ('customers', 22)]    {('help', 217), ('support', 140), ('thanks', 22), ('hands', 20), ('comfort', 10), ('hand', 9), ('relief', 7), ('supports', 2), ('accommodations', 1), ('recourse', 1), ('boost', 1), }\n",
      "'_workload', 217: [('workload', 90), ('heavy', 34), ('increased', 10), ('high', 8), ('much', 7), ('large', 4), ('overwhelming', 4), ('manageable', 4), ('big', 4), ('due', 3)]    {('workload', 215), ('workloads', 2), }\n",
      "'_patients', 277: [('care', 23), ('patients', 20), ('helping', 19), ('help', 14), ('many', 12), ('get', 8), ('helped', 7), ('happy', 7), ('love', 5), ('loss', 5)]    {('patients', 207), ('patient', 70), }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'_business', 217: [('slow', 36), ('business', 16), ('good', 13), ('growth', 11), ('new', 10), ('growing', 10), ('well', 9), ('owner', 5), ('lack', 4), ('running', 4)]    {('business', 204), ('businesses', 5), ('firm', 4), ('agency', 2), ('agencies', 1), ('corporate', 1), }\n",
      "'_staff', 291: [('not_enough', 21), ('staff', 16), ('short', 9), ('good', 9), ('support', 8), ('lack', 7), ('work', 6), ('members', 6), ('great', 5), ('new', 4)]    {('staff', 189), ('office', 100), ('offices', 2), }\n",
      "'_stress', 165: [('stress', 59), ('no', 16), ('much', 10), ('low', 9), ('high', 7), ('work', 6), ('less', 5), ('not', 3), ('lot', 3), ('job,', 3)]    {('stress', 163), ('stresses', 2), }\n",
      "'_nothing', 577: [('always', 45), ('nothing', 38), ('new', 31), ('enjoy', 20), ('something', 14), (\"i'm\", 14), ('accomplished', 14), ('specific', 12), ('good', 12), ('ever', 11)]    {('nothing', 161), ('something', 345), ('anything', 71), }\n",
      "'_raise', 175: [('got', 37), ('no', 33), ('raise', 32), ('pay', 25), ('received', 12), ('not_get', 4), ('lack', 4), ('get', 4), ('denied', 3), ('not_received', 2)]    {('raise', 157), ('raises', 18), }\n",
      "'_supervisor', 222: [('supervisor', 37), ('immediate', 8), ('harassment', 6), ('always', 6), ('new', 5), ('great', 4), ('direct', 4), ('bad', 4), ('complimented', 4), ('often', 4)]    {('supervisor', 156), ('supervisors', 48), ('director', 15), ('counselor', 1), ('foremen', 1), ('directors', 1), }\n",
      "'_fun', 155: [('fun', 96), ('job', 55), ('work', 52), ('coworkers', 13), ('place', 10), ('make', 6), ('environment', 6), ('no', 5), ('working', 5), ('people', 4)]    {('fun', 155), }\n",
      "'_children', 186: [('working', 15), ('helping', 12), ('children', 11), ('help', 10), ('work', 7), ('teaching', 6), ('see', 6), ('like', 6), ('watching', 6), ('make', 6)]    {('children', 153), ('child', 33), }\n",
      "'_responsibilities', 310: [('lot', 33), ('many', 25), ('responsibility', 24), ('much', 20), ('new', 19), ('lots', 18), ('job', 16), ('responsibilities', 10), ('work', 6), ('given', 5)]    {('responsibilities', 148), ('responsibility', 132), ('requirements', 27), ('requirement', 3), }\n",
      "'_difference', 173: [('making', 66), ('make', 44), ('made', 11), ('makes', 11), ('work', 7), ('positive', 5), ('tasks', 4), ('things', 4), ('no', 3), ('opinions', 2)]    {('difference', 147), ('variety', 21), ('inequality', 2), ('differences', 2), ('discrepancies', 1), }\n",
      "'_load', 170: [('work', 146), ('patient', 2), ('heavy', 2), ('job', 2), ('load', 2), ('delivered', 2), ('new', 2), ('information', 2), ('patients', 2), ('manual', 2)]    {('load', 146), ('overload', 17), ('loads', 7), }\n",
      "'_problems', 226: [('solve', 20), ('solving', 16), ('no', 13), ('solved', 9), ('work', 8), ('problems', 6), ('coworkers', 5), ('fix', 5), ('lot', 4), ('new', 4)]    {('problems', 144), ('problem', 82), }\n",
      "'_year', 269: [('time', 25), ('end', 17), ('new', 11), ('many', 10), ('3', 9), ('30', 8), ('last', 7), ('4', 7), ('2', 7), ('10', 7)]    {('year', 143), ('years', 126), }\n",
      "'_place', 270: [('work', 50), ('year', 25), ('day', 25), ('great', 22), ('good', 17), ('workload', 16), ('nice', 14), ('dead', 14), ('expectations', 14), ('sucks', 12)]    {('place', 142), ('end', 87), ('places', 10), ('ends', 8), ('zone', 6), ('target', 6), ('stop', 5), ('peak', 3), ('high', 1), ('targets', 1), ('stops', 1), }\n",
      "'_pressure', 164: [('pressure', 32), ('much', 14), ('no', 14), ('lot', 13), ('job', 10), ('get', 9), ('high', 7), ('meet', 6), ('achieve', 4), ('company', 4)]    {('pressure', 141), ('head', 14), ('pressures', 5), ('heads', 4), }\n",
      "'_sales', 165: [('good', 16), ('sales', 11), ('low', 9), ('no', 9), ('made', 8), ('goals', 6), ('lack', 5), ('big', 5), ('quota', 4), ('meet', 4)]    {('sales', 137), ('sale', 28), }\n",
      "'_family', 147: [('like', 23), ('away', 8), ('family', 7), ('members', 6), ('providing', 5), ('helping', 4), ('great', 3), ('money', 3), ('supporting', 3), ('member', 3)]    {('family', 128), ('families', 19), }\n",
      "'_changes', 321: [('no', 16), ('changes', 15), ('work', 13), ('many', 13), ('job', 12), ('death', 11), ('pay', 9), ('company', 8), ('change', 8), ('make', 7)]    {('changes', 128), ('change', 63), ('break', 26), ('death', 24), ('breaks', 24), ('increase', 23), ('development', 8), ('transition', 6), ('surprise', 3), ('increases', 3), ('decrease', 2), ('surprises', 2), ('birth', 2), ('deaths', 2), ('conversions', 1), ('damages', 1), ('deformation', 1), ('variation', 1), ('transformation', 1), }\n",
      "'_life', 194: [('life', 15), (\"people's\", 14), ('save', 12), ('saving', 9), ('work', 9), (\"someone's\", 8), ('better', 5), ('change', 5), ('saved', 5), ('difference', 5)]    {('life', 125), ('lives', 69), }\n",
      "'_communication', 164: [('lack', 62), ('poor', 23), ('no', 11), ('traffic', 5), ('communication', 4), ('departments', 4), ('people', 4), ('new', 3), ('lacking', 3), ('good', 3)]    {('communication', 125), ('traffic', 16), ('contact', 7), ('mail', 6), ('media', 5), ('communications', 3), ('channels', 1), ('discussions', 1), }\n",
      "'_issues', 139: [('resolve', 7), ('health', 6), ('solve', 5), ('computer', 5), ('personal', 4), ('many', 4), ('problems', 4), ('financial', 3), ('staffing', 3), ('working', 3)]    {('issues', 123), ('issue', 16), }\n",
      "'_love', 124: [('job', 635), ('love', 222), ('work', 108), ('working', 69), ('people', 60), ('helping', 58), ('coworkers', 34), ('teaching', 22), ('customers', 14), ('taking', 12)]    {('love', 121), ('loyalty', 3), }\n",
      "'_workers', 173: [('workers', 16), ('not_enough', 9), ('hard', 9), ('good', 6), ('manager', 6), ('work', 5), ('lazy', 5), ('much', 4), ('great', 4), ('best', 3)]    {('workers', 116), ('worker', 23), ('drivers', 8), ('assistant', 5), ('driver', 5), ('assistants', 4), ('temp', 4), ('volunteers', 3), ('volunteer', 2), ('freelancer', 1), ('collectors', 1), ('workmates', 1), }\n",
      "'_one', 125: [('no', 102), ('day', 40), ('work', 26), ('thing', 22), ('job', 18), (\"i'm\", 12), ('much', 8), ('one', 8), ('things', 7), ('people', 7)]    {('one', 115), ('ones', 9), ('every', 1), }\n",
      "'_training', 118: [('lack', 18), ('no', 8), ('training', 6), ('not_enough', 5), ('new', 5), ('employee', 5), ('issues', 4), ('proper', 3), ('people', 3), ('required', 2)]    {('training', 113), ('discipline', 2), ('exercise', 1), ('trainings', 1), ('drill', 1), }\n",
      "'_promotion', 126: [('promotion', 38), ('passed', 12), ('got', 10), ('possible', 6), ('not_get', 5), ('no', 4), ('chance', 3), ('getting', 3), ('received', 2), ('looked', 2)]    {('promotion', 113), ('promotions', 12), ('ad', 1), }\n",
      "'_amount', 183: [('work', 51), ('short', 11), ('hours', 8), ('tasks', 6), ('patient', 6), ('clients', 6), ('job', 5), ('income', 5), ('opportunity', 5), ('time', 4)]    {('amount', 102), ('loss', 39), ('contributions', 11), ('payroll', 7), ('contribution', 6), ('coverage', 5), ('gain', 3), ('advance', 2), ('amounts', 2), ('figure', 2), ('advances', 1), ('deductibles', 1), ('gains', 1), ('losses', 1), }\n",
      "'_none', 100: [('none', 91), ('stress', 2), ('employees', 2), ('supervisors', 2), ('coworkers', 2), ('stop', 2), ('violence', 2), ('added', 1), ('since', 1), ('rights', 1)]    {('none', 100), }\n",
      "'_sleep', 95: [('not_enough', 31), ('lack', 26), ('no', 14), ('not', 6), ('sleep', 4), ('enough', 4), ('need', 3), ('little', 1), ('job', 1), ('time', 1)]    {('sleep', 95), }\n",
      "'_part', 222: [('team', 31), ('job', 25), ('daily', 12), ('family', 9), ('time', 8), ('work', 7), ('staff', 6), ('part', 4), ('get', 4), ('not_doing', 3)]    {('part', 94), ('members', 39), ('basis', 17), ('parts', 16), ('member', 16), ('items', 15), ('details', 9), ('unit', 7), ('detail', 5), ('butt', 2), ('item', 2), }\n",
      "'_results', 221: [('good', 23), ('see', 16), ('new', 14), ('results', 10), ('positive', 10), ('great', 8), ('end', 7), ('finished', 7), ('seeing', 5), ('work', 5)]    {('results', 90), ('product', 60), ('products', 34), ('impact', 15), ('result', 14), ('influence', 5), ('repercussions', 2), ('harvest', 1), }\n",
      "'_success', 100: [('success', 34), ('student', 7), ('good', 4), ('seeing', 4), ('teaching', 3), ('team', 3), ('students', 3), ('teams', 2), ('saw', 2), ('work', 2)]    {('success', 90), ('successes', 10), }\n",
      "'_opportunities', 211: [('new', 35), ('no', 22), ('advancement', 8), ('opportunity', 8), ('lots', 7), ('lack', 7), ('opportunities', 7), ('learning', 6), ('job', 6), ('many', 5)]    {('opportunities', 89), ('opportunity', 71), ('room', 33), ('opening', 5), ('occasions', 3), ('say', 2), ('hearing', 2), ('hearings', 1), ('shots', 1), ('rooms', 1), ('openings', 1), ('shot', 1), ('occasion', 1), }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'_expectations', 109: [('high', 11), ('unrealistic', 9), ('expectations', 4), ('anticipation', 4), ('meeting', 3), ('unreasonable', 3), ('job', 3), ('exceeding', 3), ('exceeded', 3), ('meet', 3)]    {('expectations', 87), ('expectation', 7), ('anticipation', 6), ('possibility', 4), ('promises', 3), ('possibilities', 2), }\n",
      "'_shift', 134: [('long', 12), ('hour', 11), ('night', 9), ('3rd', 6), ('work', 6), ('shift', 5), ('graveyard', 4), ('end', 3), ('whole', 3), ('third', 3)]    {('shift', 85), ('shifts', 49), }\n",
      "'_experience', 95: [('experience', 17), ('good', 7), ('years', 7), ('new', 6), ('knowledge', 3), ('much', 3), ('work', 2), ('people', 2), ('no', 2), ('less', 2)]    {('experience', 82), ('experiences', 13), }\n",
      "'_decisions', 185: [('make', 15), ('phone', 10), ('made', 8), ('making', 8), ('many', 8), ('people', 8), ('management', 7), ('work', 7), ('lot', 6), ('bad', 5)]    {('decisions', 81), ('calls', 37), ('call', 33), ('decision', 25), ('move', 6), ('moves', 3), }\n",
      "'_performance', 158: [('good', 18), ('benefits', 12), ('job', 11), ('work', 11), ('no', 7), ('people', 6), ('poor', 5), ('worried', 4), ('performance', 4), ('pay', 4)]    {('performance', 80), ('benefits', 69), ('benefit', 6), ('concert', 1), ('act', 1), ('concerts', 1), }\n",
      "'_bonus', 91: [('bonus', 9), ('got', 9), ('christmas', 7), ('no', 5), ('holiday', 4), ('raise', 4), ('raises', 3), ('good', 3), ('received', 3), ('extra', 3)]    {('bonus', 77), ('bonuses', 14), }\n",
      "'_feedback', 77: [('positive', 31), ('good', 7), ('feedback', 4), ('managerial', 3), ('great', 3), ('owner', 3), ('customer', 3), ('get', 2), ('no', 2), ('site', 2)]    {('feedback', 77), }\n",
      "'_schedule', 90: [('work', 8), ('flexible', 7), ('schedule', 7), ('changes', 4), ('set', 4), ('school', 3), ('time', 3), ('ahead', 3), ('busy', 2), ('flexibility', 2)]    {('schedule', 76), ('schedules', 9), ('menu', 4), ('menus', 1), }\n",
      "'_week', 97: [('hours', 18), ('days', 11), ('work', 10), ('per', 7), ('every', 5), ('week', 5), ('day', 3), ('end', 3), ('3', 3), ('hour', 2)]    {('week', 75), ('weeks', 22), }\n",
      "'_friends', 104: [('work', 28), ('friends', 14), ('good', 14), ('new', 4), ('see', 4), ('family', 3), ('workplace', 2), ('fired', 2), ('no', 2), ('many', 2)]    {('friends', 74), ('friend', 14), ('light', 4), ('lights', 3), ('buddies', 2), ('companions', 2), ('brother', 1), ('mates', 1), ('brothers', 1), ('mate', 1), ('girlfriend', 1), }\n",
      "'_skills', 92: [('new', 29), ('using', 3), ('different', 2), ('specific', 2), ('make', 2), ('skills', 2), ('recognized', 2), ('people', 2), ('help', 2), ('life', 2)]    {('skills', 73), ('skill', 17), ('craft', 1), ('crafts', 1), }\n",
      "'_stuff', 99: [('got', 8), ('get', 7), ('much', 7), ('lot', 6), ('work', 6), ('time', 4), ('stuff', 3), ('heavy', 3), ('fun', 2), ('new', 2)]    {('stuff', 72), ('paper', 10), ('papers', 8), ('waste', 5), ('fluff', 1), ('rock', 1), ('dust', 1), ('chemicals', 1), }\n",
      "'_security', 78: [('job', 44), ('security', 8), ('peace', 3), ('lack', 2), ('consulting', 2), ('manager', 2), ('system', 2), ('mind', 2), ('financial', 1), ('new', 1)]    {('security', 72), ('peace', 6), }\n",
      "'_accomplishment', 163: [('accomplishment', 39), ('accomplishments', 36), ('things', 29), ('not', 27), ('lot', 15), ('sense', 12), ('work', 12), ('much', 12), ('job', 9), ('well', 8)]    {('accomplishment', 72), ('accomplishments', 56), ('credit', 24), ('record', 4), ('arrival', 2), ('records', 1), ('arrivals', 1), ('going', 1), ('credits', 1), ('masterpiece', 1), }\n",
      "'_recognition', 70: [('no', 24), ('recognition', 7), ('not_get', 5), ('received', 3), ('lack', 3), ('little', 3), ('management', 2), ('work', 2), ('leaders', 2), ('much', 2)]    {('recognition', 68), ('recognitions', 2), }\n",
      "'_progress', 68: [('make', 8), ('made', 8), ('making', 7), ('lack', 5), ('progress', 5), ('no', 4), ('see', 3), ('patient', 3), ('work', 3), ('student', 3)]    {('progress', 67), ('strides', 1), }\n",
      "'_department', 89: [('communication', 4), ('work', 3), ('understaffed,', 2), ('within', 2), ('another', 2), ('decision', 2), ('11', 2), ('works', 2), ('currently', 2), ('friendly', 2)]    {('department', 66), ('departments', 16), ('dept', 7), }\n",
      "'_rules', 74: [('change', 11), ('new', 5), ('not_follow', 5), ('rules', 4), ('follow', 4), ('many', 3), ('changing', 3), ('seem', 2), ('enforced', 2), ('regulations', 2)]    {('rules', 65), ('restrictions', 7), ('rule', 1), ('restriction', 1), }\n",
      "'_bosses', 64: [('bosses', 4), ('many', 4), ('new', 3), ('actively', 2), ('comments', 2), ('huge', 2), ('gave', 2), ('horrible', 2), ('talk', 2), ('think', 2)]    {('bosses', 64), }\n",
      "'_field', 65: [('love', 4), ('work', 4), ('working', 4), ('enjoy', 4), ('degree', 3), ('field', 3), ('job', 3), ('experience', 2), ('best', 2), ('long', 2)]    {('field', 64), ('campus', 1), }\n",
      "'_overtime', 62: [('overtime', 14), ('working', 8), ('lot', 6), ('work', 6), ('mandatory', 4), ('hours', 3), ('weekend', 2), ('no', 2), ('long', 2), ('lots', 2)]    {('overtime', 62), }\n",
      "'_control', 66: [('no', 9), ('not', 9), ('things', 7), ('control', 6), ('lack', 5), ('work', 5), ('beyond', 4), ('lot', 3), ('five', 2), ('cannot', 2)]    {('control', 59), ('authority', 5), ('hold', 1), ('controls', 1), }\n",
      "'_challenges', 103: [('new', 29), ('no', 21), ('challenge', 6), ('lack', 5), ('like', 4), ('daily', 3), ('challenges', 3), ('love', 3), ('occurred', 2), ('many', 2)]    {('challenges', 58), ('challenge', 45), }\n",
      "'_information', 104: [('new', 6), ('incomplete', 3), ('job', 3), ('need', 3), ('lack', 3), ('good', 3), ('correct', 2), ('enough', 2), ('overload', 2), ('seemed', 2)]    {('information', 58), ('materials', 14), ('material', 9), ('news', 9), ('fact', 8), ('misinformation', 2), ('database', 1), ('confirmation', 1), ('facts', 1), ('intelligence', 1), }\n",
      "'_income', 63: [('income', 10), ('low', 5), ('loss', 4), ('source', 3), ('work', 3), ('making', 2), ('steady', 2), ('based', 2), ('raise', 2), ('went', 2)]    {('income', 57), ('return', 3), ('returns', 2), ('incomes', 1), }\n",
      "'_season', 56: [('holiday', 21), ('busy', 9), ('slow', 5), ('peak', 3), ('right', 2), ('tax', 2), ('new', 2), ('christmas', 2), ('almost', 2), ('paced', 1)]    {('season', 55), ('seasons', 1), }\n",
      "'_satisfaction', 84: [('satisfaction', 13), ('take', 12), ('job', 10), ('gives', 4), ('sense', 3), ('get', 3), ('customer', 3), ('lot', 3), ('great', 3), ('task', 3)]    {('satisfaction', 53), ('pride', 25), ('fulfillment', 6), }\n",
      "'_demands', 75: [('high', 11), ('job', 9), ('many', 7), ('demands', 6), ('work', 5), ('management', 3), ('unreasonable', 3), ('meet', 3), ('demand', 3), ('students', 2)]    {('demands', 52), ('demand', 23), }\n",
      "'_school', 54: [('high', 6), ('work', 5), ('new', 4), ('bus', 4), ('going', 3), ('went', 3), ('back', 2), ('district', 2), ('around', 2), ('busses', 2)]    {('school', 52), ('schools', 2), }\n",
      "'_feet', 56: [('day', 16), ('standing', 6), (\"i'm\", 6), ('hurt', 4), ('time', 4), ('hours', 3), ('always', 3), ('lot', 3), ('days', 2), ('feet', 2)]    {('feet', 52), ('foot', 4), }\n",
      "'_health', 52: [('issues', 8), ('health', 5), ('insurance', 4), ('mental', 3), ('due', 2), ('concerns', 2), ('good', 2), ('bad', 2), ('major', 2), ('care', 2)]    {('health', 52), }\n",
      "'_colleagues', 57: [('good', 6), ('colleagues', 4), ('incompetent', 3), ('job', 2), ('among', 2), ('passed', 2), ('wolves', 2), ('excellent', 2), ('negative', 2), ('value', 2)]    {('colleagues', 52), ('colleague', 5), }\n",
      "'_knowledge', 271: [('content', 16), ('work', 14), ('new', 8), ('knowledge', 8), ('best', 7), ('experience', 6), ('job', 6), ('lack', 6), ('no', 6), ('get', 6)]    {('knowledge', 51), ('attitude', 34), ('ability', 31), ('process', 23), ('content', 21), ('processes', 18), ('mind', 17), ('attitudes', 16), ('abilities', 14), ('structure', 14), ('inability', 10), ('practices', 7), ('practice', 6), ('history', 3), ('minds', 3), ('histories', 2), ('contents', 1), }\n",
      "'_direction', 78: [('lack', 10), ('no', 6), ('not_follow', 5), ('not_following', 4), ('many', 4), ('wrong', 3), ('clear', 3), ('company', 3), ('new', 2), ('not_much', 2)]    {('direction', 51), ('directions', 22), ('course', 4), ('courses', 1), }\n",
      "'_future', 51: [('future', 7), ('uncertain', 5), ('near', 4), ('not_sure', 3), ('forward', 3), ('unsure', 3), ('company', 3), ('secure', 3), ('skiers', 2), ('role', 2)]    {('future', 50), ('futures', 1), }\n",
      "'_salary', 49: [('salary', 11), ('low', 8), ('good', 5), ('decent', 2), ('job', 2), ('since', 2), ('got', 2), ('less', 2), ('enough', 2), ('great', 1)]    {('salary', 49), }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'_growth', 63: [('room', 5), ('business', 5), ('no', 5), ('students', 4), ('saw', 3), ('see', 3), ('culture', 3), ('opportunity', 2), ('growth', 2), ('seeing', 2)]    {('growth', 49), ('culture', 11), ('teething', 1), ('cohesion', 1), ('cultures', 1), }\n",
      "'_respect', 50: [('lack', 17), ('no', 14), ('not', 9), ('respect', 3), ('treated', 2), ('coworkers,', 2), ('people', 2), ('shown', 2), ('speak', 1), ('not_treated', 1)]    {('respect', 49), ('respects', 1), }\n",
      "'_paycheck', 54: [('paycheck', 25), ('nice', 3), ('getting', 3), ('get', 2), ('made', 2), ('got', 2), ('dispute', 2), ('paychecks', 2), ('continuous', 1), ('received', 1)]    {('paycheck', 48), ('paychecks', 6), }\n",
      "'_bills', 50: [('pay', 18), ('pays', 5), ('paying', 4), ('always', 2), ('not_pay', 2), ('overwhelming', 2), ('bills', 2), ('cover', 2), ('understand', 1), ('unsure', 1)]    {('bills', 47), ('bill', 3), }\n",
      "'_mistakes', 91: [('made', 18), ('make', 12), ('making', 8), ('mistakes', 7), ('stupidity', 5), ('coworkers', 5), ('others', 3), ('confusion', 3), ('fix', 2), (\"people's\", 2)]    {('mistakes', 47), ('mistake', 29), ('stupidity', 9), ('oversight', 3), ('confusion', 3), }\n",
      "'_employer', 63: [('employer', 6), ('great', 6), ('happy', 3), ('want', 3), ('taken', 2), ('requiring', 2), ('depends', 2), ('pleased', 2), ('put', 2), ('offers', 2)]    {('employer', 47), ('employers', 11), ('masters', 4), ('master', 1), }\n",
      "'_organization', 110: [('bureaucracy', 6), ('christmas', 6), ('getting', 4), ('lack', 4), ('work', 3), ('people', 3), ('politics', 3), ('workplace', 3), ('changes', 2), ('good', 2)]    {('organization', 47), ('bureaucracy', 15), ('party', 15), ('machine', 9), ('machines', 8), ('parties', 4), ('institution', 2), ('organizations', 2), ('forces', 2), ('union', 2), ('host', 1), ('association', 1), ('grey', 1), ('force', 1), }\n",
      "'_sense', 46: [('accomplishment', 20), ('common', 6), ('not_make', 5), ('fun', 4), ('gives', 3), ('get', 2), ('pride', 2), ('learning', 2), ('independence', 2), ('purpose', 2)]    {('sense', 46), }\n",
      "'_store', 50: [('closing', 6), ('busy', 3), ('slow', 3), ('new', 3), ('sells', 2), ('retail', 2), ('gets', 2), ('employees', 2), ('conditions', 2), ('come', 2)]    {('store', 46), ('stores', 3), ('salon', 1), }\n",
      "'_system', 63: [('new', 7), ('computer', 6), ('outages', 2), ('support', 2), ('issues', 2), ('went', 2), ('confusing', 2), ('security', 2), ('problems', 2), ('set', 2)]    {('system', 45), ('systems', 13), ('network', 5), }\n",
      "'_needs', 89: [('money', 25), ('get', 20), ('people', 19), ('things', 14), ('need', 12), ('help', 10), ('special', 9), ('work', 9), ('job', 8), ('meet', 6)]    {('needs', 45), ('need', 44), }\n",
      "'_level', 65: [('pay', 90), ('stress', 16), ('salary', 11), ('sales', 9), ('volume', 9), ('pay,', 8), ('income', 7), ('hours', 5), ('knowledge', 4), ('responsibility', 4)]    {('level', 45), ('levels', 8), ('low', 6), ('depth', 2), ('grind', 2), ('qualities', 2), }\n",
      "'_owner', 74: [('business', 7), ('feedback', 6), ('new', 6), ('owner', 4), ('make', 4), ('not_care', 3), ('told', 2), ('conflict', 2), ('completely', 2), ('called', 2)]    {('owner', 44), ('owners', 25), ('letter', 4), ('letters', 1), }\n",
      "'_leadership', 54: [('poor', 10), ('lack', 5), ('no', 3), ('new', 3), ('bad', 3), ('team', 3), ('temporary', 2), ('show', 2), ('recognition', 2), ('horrible', 2)]    {('leadership', 44), ('lead', 7), ('leads', 3), }\n",
      "'_resources', 46: [('lack', 7), ('not_enough', 6), ('time', 4), ('resources', 3), ('limited', 3), ('not_have', 2), ('required', 2), ('given', 2), ('obtaining', 1), ('not_provided', 1)]    {('resources', 44), ('resource', 2), }\n",
      "'_orders', 75: [('lot', 4), ('others', 4), ('need', 3), ('above--having', 2), ('orders', 2), ('must', 2), ('work', 2), ('many', 2), ('not_keep', 2), ('fill', 2)]    {('orders', 43), ('order', 18), ('word', 10), ('words', 4), }\n",
      "'_interaction', 61: [('people', 12), ('customer', 9), ('students', 6), ('customers', 4), ('enjoy', 4), ('social', 4), ('clients', 4), ('good', 3), ('coworker', 3), ('get', 3)]    {('interaction', 43), ('interactions', 17), ('interact', 1), }\n",
      "'_type', 84: [('work', 47), ('job', 19), ('love', 4), ('repetitive', 4), ('like', 3), ('different', 3), ('nature', 3), ('work/work', 2), ('call', 2), ('amount', 2)]    {('type', 42), ('nature', 38), ('types', 4), }\n",
      "'_routine', 44: [('routine', 29), ('work', 9), ('job', 7), ('tasks', 4), ('daily', 4), ('day', 3), ('work,', 2), ('procedures', 2), ('easy', 2), ('days', 2)]    {('routine', 42), ('ruts', 2), }\n",
      "'_layoffs', 45: [('layoffs', 17), ('occurring', 4), ('coming', 4), ('increased', 2), ('pending', 2), ('doubled', 2), ('possible', 2), ('often', 2), ('lot', 1), ('job', 1)]    {('layoffs', 41), ('layoff', 4), }\n",
      "'_atmosphere', 42: [('good', 8), ('great', 7), ('atmosphere', 5), ('friendly', 4), ('pleasant', 4), ('quiet', 3), ('like', 2), ('relaxing', 2), ('wonderful', 2), ('peaceful', 2)]    {('atmosphere', 41), ('spirit', 1), }\n",
      "'_working', 41: [('hard', 57), ('long', 52), ('working', 39), ('love', 38), ('good', 35), ('people', 34), ('many', 32), ('lot', 32), ('not', 28), ('children', 22)]    {('working', 41), }\n",
      "'_teaching', 54: [('love', 11), ('teaching', 10), ('children', 8), ('new', 8), ('class', 6), ('fun', 6), ('enjoy', 5), ('difficult', 4), ('like', 4), ('kids', 4)]    {('teaching', 40), ('lesson', 7), ('lessons', 3), ('lecture', 3), ('teachings', 1), }\n",
      "'_computer', 47: [('issues', 7), ('new', 5), ('system', 4), ('looking', 3), ('needs', 2), ('failure', 2), ('broke', 2), ('work', 2), ('malfunctions', 2), ('programming', 2)]    {('computer', 40), ('computers', 4), ('server', 2), ('servers', 1), }\n",
      "'_holiday', 70: [('season', 26), ('holidays', 13), ('busy', 8), ('work', 5), ('hours', 5), ('bonus', 4), ('time', 3), ('crowds', 2), ('no', 2), ('madness', 2)]    {('holiday', 39), ('holidays', 31), }\n",
      "'_situation', 101: [('difficult', 8), ('bad', 7), ('stressful', 7), ('acceptance', 4), ('situation', 3), ('job', 3), ('certain', 3), ('no', 3), ('client', 2), ('control', 2)]    {('situation', 39), ('situations', 38), ('complications', 6), ('acceptance', 6), ('size', 4), ('picture', 3), ('elements', 2), ('pictures', 1), ('rejection', 1), ('element', 1), }\n",
      "'_praise', 79: [('received', 6), ('no', 6), ('getting', 5), ('boss', 5), ('receive', 5), ('customer', 5), ('get', 4), ('compliments', 4), ('manager', 3), ('praise', 3)]    {('praise', 39), ('compliments', 24), ('compliment', 9), ('recommendations', 5), ('recommendation', 1), ('praises', 1), }\n",
      "'_night', 56: [('work', 10), ('late', 5), ('good', 3), ('shift', 2), ('one', 2), ('busy', 2), ('working', 2), ('meetings', 2), ('long', 2), ('hour', 2)]    {('night', 39), ('nights', 17), }\n",
      "'_residents', 52: [('pass', 8), ('need', 4), ('residents', 4), ('passed', 4), ('many', 3), ('love', 3), ('make', 3), ('care', 2), ('sure', 2), ('reside', 2)]    {('residents', 39), ('inmates', 5), ('tenants', 4), ('resident', 2), ('coaster', 1), ('tenant', 1), }\n",
      "'_equipment', 41: [('broken', 5), ('work', 3), ('breakdown', 3), ('malfunctioning', 2), ('new', 2), ('breaking', 2), ('poor', 2), ('issues', 2), ('repair', 2), ('break', 2)]    {('equipment', 39), ('gear', 2), }\n",
      "'_appreciation', 40: [('no', 8), ('lack', 4), ('customer', 3), ('shows', 2), ('appreciation', 2), ('express', 2), ('not_show', 1), ('thank', 1), ('work', 1), ('verbalized', 1)]    {('appreciation', 39), ('appreciations', 1), }\n",
      "'_advancement', 40: [('no', 15), ('room', 6), ('lack', 3), ('opportunity', 3), ('opportunities', 3), ('advancement', 2), ('growth', 1), ('limited', 1), ('not_selected', 1), ('mobility', 1)]    {('advancement', 39), ('advancements', 1), }\n",
      "'_christmas', 38: [('party', 10), ('bonus', 4), ('time', 4), ('work', 3), ('christmas', 2), ('rush', 2), ('season', 2), ('celebration', 2), ('received', 2), ('break', 2)]    {('christmas', 38), }\n",
      "'_completion', 43: [('tasks', 64), ('project', 61), ('task', 47), ('job', 39), ('work', 27), ('projects', 18), ('not', 13), ('difficult', 10), ('successfully', 10), ('big', 8)]    {('completion', 38), ('completions', 2), ('completed', 2), ('completing', 1), }\n",
      "'_line', 55: [('dead', 7), ('time', 6), ('bottom', 3), ('assembly', 2), ('love', 2), ('staff', 2), ('fast', 2), ('like', 2), ('duty', 2), ('product', 2)]    {('line', 38), ('lines', 12), ('row', 4), ('ranks', 1), }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'_workplace', 60: [('new', 5), ('drama', 4), ('politics', 3), ('location', 3), ('favoritism', 2), ('little', 2), ('party', 2), ('happy', 2), ('7', 2), ('dirty', 2)]    {('workplace', 38), ('location', 16), ('locations', 4), ('farm', 1), ('lab', 1), }\n",
      "'_effort', 82: [('best', 16), ('job', 12), ('work', 6), ('give', 6), ('scores', 6), ('try', 6), ('recognized', 5), ('make', 5), ('team', 4), ('results', 4)]    {('effort', 37), ('efforts', 18), ('test', 15), ('struggle', 4), ('tests', 3), ('bid', 2), ('struggles', 2), ('best', 1), }\n",
      "'_education', 37: [('education', 4), ('get', 2), ('experience', 2), ('important', 2), ('effects', 2), ('background', 2), ('training', 2), ('dollars', 2), ('not_care', 1), ('tax', 1)]    {('education', 36), ('extensions', 1), }\n",
      "'_pace', 36: [('fast', 14), ('work', 7), ('slow', 3), ('business', 2), ('good', 2), ('pace', 2), ('control', 1), ('technology', 1), ('handle', 1), ('se', 1)]    {('pace', 36), }\n",
      "'_reason', 64: [('no', 27), ('test', 8), ('personal', 3), ('reasons', 3), ('many', 3), ('financial', 1), ('special', 1), ('rhyme', 1), ('large', 1), ('inferior', 1)]    {('reason', 35), ('reasons', 17), ('scores', 9), ('score', 3), }\n",
      "'_behavior', 45: [('student', 8), ('students', 5), ('behavior', 3), ('employee', 2), ('coworkers', 2), ('others', 2), ('high', 2), ('sub', 2), ('clients', 1), ('horrible', 1)]    {('behavior', 35), ('behaviors', 10), }\n",
      "'_good', 35: [('job', 417), ('good', 224), ('work', 181), ('people', 79), ('pay', 79), ('feel', 58), ('coworkers', 56), (\"i'm\", 51), ('day', 48), ('environment', 43)]    {('good', 35), }\n",
      "'_feeling', 148: [('like', 14), ('passion', 12), ('good', 11), ('not', 9), ('accomplishment', 8), ('stuck', 8), ('hope', 8), ('work', 7), ('important', 6), ('no', 6)]    {('feeling', 34), ('passion', 18), ('pain', 18), ('emotions', 16), ('feelings', 11), ('hope', 9), ('happiness', 8), ('gratitude', 7), ('pleasure', 5), ('emotion', 5), ('apathy', 3), ('desire', 3), ('desires', 1), ('affect', 1), ('passions', 1), ('sex', 1), ('sadness', 1), ('soul', 1), ('enthusiasm', 1), ('gravity', 1), ('sympathy', 1), ('hopes', 1), ('complexes', 1), }\n",
      "'_questions', 42: [('answers', 5), ('asking', 4), ('answer', 3), ('asked', 3), ('simple', 2), ('never', 2), ('stupid', 2), ('important', 2), ('clients', 2), ('mostly', 1)]    {('questions', 34), ('question', 8), }\n",
      "'_meeting', 60: [('deadlines', 26), ('people', 16), ('new', 14), ('not', 13), ('goals', 12), ('different', 6), ('sales', 4), ('exceeding', 4), ('love', 4), ('quotas', 4)]    {('meeting', 34), ('meetings', 25), ('congress', 1), }\n",
      "'_drama', 33: [('drama', 8), ('no', 3), ('much', 3), ('work', 3), ('queen', 2), ('free', 2), ('workplace', 2), ('unnecessary', 1), ('inappropriate', 1), ('job', 1)]    {('drama', 32), ('dramas', 1), }\n",
      "'_public', 32: [('general', 5), ('dealing', 5), ('working', 4), ('deal', 4), ('interaction', 3), ('sales', 2), ('service', 2), ('controversy', 2), ('work', 2), ('hard', 2)]    {('public', 32), }\n",
      "'_wage', 46: [('minimum', 12), ('no', 3), ('low', 3), ('less', 3), ('increases', 2), ('small', 2), ('min', 2), ('minimal', 1), ('raise', 1), ('min.', 1)]    {('wage', 32), ('wages', 14), }\n",
      "'_self', 31: [('explanatory', 6), ('managed', 4), ('motivated', 2), ('manage', 2), ('confidence', 2), ('employment', 2), ('self', 2), ('low', 2), ('worth', 2), ('self,', 1)]    {('self', 31), }\n",
      "'_politics', 30: [('politics', 7), ('office', 6), ('work', 4), ('workplace', 3), ('job', 2), ('discussed', 2), ('much', 2), ('association', 1), ('\"office\"', 1), ('due', 1)]    {('politics', 30), }\n",
      "'_volume', 36: [('high', 9), ('work', 8), ('low', 5), ('much', 3), ('call', 2), ('sometimes', 2), ('large', 2), ('workers/high', 1), ('volume', 1), ('customer', 1)]    {('volume', 30), ('volumes', 3), ('capacity', 3), }\n",
      "'_ideas', 110: [('new', 14), ('no', 7), ('work', 4), ('lesson', 4), ('great', 3), ('heard', 2), ('processed', 2), ('coming', 2), ('creative', 2), ('taken', 2)]    {('ideas', 30), ('plan', 18), ('plans', 14), ('concept', 11), ('idea', 10), ('suggestions', 7), ('concepts', 6), ('meaning', 6), ('reactions', 2), ('reaction', 2), ('suggestion', 1), ('burdens', 1), ('themes', 1), ('notions', 1), }\n",
      "'_area', 89: [('work', 12), ('call', 5), ('provide', 3), ('expertise', 2), ('employees', 2), ('important', 2), ('director', 2), ('buzz', 2), ('many', 2), ('office', 2)]    {('area', 30), ('safety', 17), ('areas', 12), ('center', 10), ('space', 8), ('scene', 2), ('danger', 2), ('blocks', 2), ('centers', 2), ('retreat', 1), ('dangers', 1), ('block', 1), ('corners', 1), }\n",
      "'_compensation', 29: [('compensation', 7), ('no', 4), ('good', 3), ('without', 2), ('additional', 2), ('unnecessary', 2), ('better', 1), ('monetary', 1), ('inadequate', 1), ('little', 1)]    {('compensation', 29), }\n",
      "'_phone', 36: [('calls', 6), ('people', 2), ('orders,', 2), ('rang', 2), ('work', 2), ('constantly', 2), ('stolen', 2), ('system', 2), ('ringing', 2), ('ring', 2)]    {('phone', 29), ('phones', 7), }\n",
      "'_aspects', 49: [('new', 5), ('certain', 3), ('enjoy', 3), ('many', 3), ('work', 3), ('job', 2), ('clinic,', 2), ('creative', 1), ('several', 1), ('control', 1)]    {('aspects', 29), ('side', 11), ('aspect', 8), ('sector', 1), }\n",
      "'_learning', 30: [('new', 104), ('learning', 14), ('always', 11), ('students', 10), (\"i'm\", 7), ('lot', 6), ('not', 5), ('kids', 4), ('opportunities', 4), ('people', 4)]    {('learning', 28), ('study', 2), }\n",
      "'_production', 28: [('standards', 2), ('production', 2), ('increased', 2), ('met', 2), ('lack', 2), ('date', 2), ('good', 2), ('attitude', 1), ('monthly', 1), ('continue', 1)]    {('production', 28), }\n",
      "'_animals', 55: [('work', 7), ('love', 3), ('people', 3), ('helping', 2), ('help', 2), ('working', 2), ('animals', 2), ('die', 2), ('seeing', 2), ('funny', 2)]    {('animals', 28), ('adults', 10), ('pets', 5), ('males', 4), ('pet', 2), ('animal', 2), ('females', 1), ('adult', 1), ('male', 1), ('female', 1), }\n",
      "'_food', 33: [('need', 3), ('good', 2), ('great', 2), ('fast', 2), ('atmosphere', 2), ('dead', 2), ('people', 1), ('put', 1), ('quality', 1), ('serving', 1)]    {('food', 28), ('water', 4), ('foods', 1), }\n",
      "'_potential', 33: [('full', 8), ('potential', 6), ('new', 3), ('layoff', 2), ('lot', 2), ('earnings', 2), ('outsourcing', 2), ('earning', 1), ('not_used', 1), ('working', 1)]    {('potential', 28), ('prospects', 2), ('prospect', 2), ('potentials', 1), }\n",
      "'_insurance', 29: [('health', 9), ('companies', 4), ('challenging', 2), ('provided', 2), ('speak', 1), ('work', 1), ('new', 1), ('make', 1), ('systems', 1), ('dealing', 1)]    {('insurance', 27), ('insurances', 1), ('assurance', 1), }\n",
      "'_charge', 28: [('people', 5), ('charge', 5), (\"i'm\", 4), ('many', 4), ('getting', 2), ('myself,', 2), ('sales', 2), ('destiny', 2), ('feel', 1), ('left', 1)]    {('charge', 27), ('charges', 1), }\n",
      "'_industry', 35: [('work', 3), ('innovations', 2), (\"i'm\", 2), ('want', 2), ('uncertainty', 2), ('changing', 2), ('hit', 2), ('leader', 2), ('reading', 1), ('new', 1)]    {('industry', 27), ('market', 6), ('aviation', 1), ('markets', 1), }\n",
      "'_tools', 30: [('proper', 3), ('not_always', 3), ('made', 2), ('not_have', 2), ('not_provide', 2), ('created', 2), ('need', 2), (\"i'm\", 2), ('society', 2), ('enough', 1)]    {('tools', 27), ('tool', 2), ('gang', 1), }\n",
      "'_music', 29: [('teach', 4), ('listen', 3), ('music', 2), ('fun', 2), ('singing', 2), ('making', 1), ('playing', 1), ('work', 1), ('no', 1), ('great', 1)]    {('music', 27), ('tune', 1), ('bach', 1), }\n",
      "'_scheduling', 26: [('issues', 4), ('scheduling', 3), ('errors', 2), ('causes', 2), ('conflicts', 2), ('pain', 2), ('finances', 2), ('creates', 1), ('time', 1), ('specifically', 1)]    {('scheduling', 26), }\n",
      "'_policies', 52: [('changes', 5), ('company', 4), ('policy', 4), ('work', 3), ('place', 2), ('new', 2), ('sell', 2), ('go', 1), ('changing', 1), ('unfair', 1)]    {('policies', 26), ('policy', 26), }\n",
      "'_repetition', 29: [('repetition', 16), ('tasks', 8), ('sometimes', 1), ('lot', 1), ('occasional', 1), ('much', 1), ('lots', 1), ('plod', 1), ('cycle', 1), ('menstrual', 1)]    {('repetition', 26), ('cycle', 3), }\n",
      "'_procedures', 39: [('new', 4), ('changes', 2), ('place', 2), ('meaningless', 2), ('ridiculous', 2), ('causes', 2), ('went', 2), ('certain', 1), ('clear', 1), ('not_follow', 1)]    {('procedures', 26), ('condition', 7), ('procedure', 6), }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'_vacation', 28: [('need', 4), ('vacation', 4), ('take', 2), ('starts', 2), ('work', 2), ('hours,', 1), ('weeks', 1), ('no', 1), ('wanted', 1), ('days', 1)]    {('vacation', 26), ('vacations', 2), }\n",
      "'_parents', 34: [('parents', 3), ('patients', 2), ('coworkers', 2), ('make', 2), ('seem', 2), ('thanked', 2), ('either', 2), ('supervisors', 2), ('abuse', 2), ('funny', 2)]    {('parents', 25), ('parent', 7), ('mother', 1), ('father', 1), }\n",
      "'_months', 42: [('busy', 3), ('6', 3), ('3', 3), ('good', 2), ('7', 2), ('three', 2), ('one', 2), ('goals', 2), ('recent', 1), ('bandwidth', 1)]    {('months', 25), ('month', 17), }\n",
      "'_weather', 25: [('weather', 4), ('icy', 2), ('would', 2), ('nature', 2), ('winter', 2), ('put', 2), ('bad', 2), ('conditions', 2), ('cooler', 2), ('cold', 2)]    {('weather', 25), }\n",
      "'_budget', 28: [('cuts', 8), ('budget', 3), ('time', 3), ('change', 2), ('reasons', 2), ('concerns', 2), ('hit', 2), ('made', 2), ('not_enough', 1), ('time,', 1)]    {('budget', 25), ('budgets', 3), }\n",
      "'_answers', 32: [('not_getting', 2), ('finding', 2), ('see', 2), ('lack', 1), ('someone', 1), ('customers', 1), ('questions', 1), ('found', 1), ('school', 1), ('listen', 1)]    {('answers', 24), ('answer', 8), }\n",
      "'_group', 77: [('great', 5), ('helping', 4), ('service', 4), ('good', 3), ('things', 3), ('completes', 2), ('leader', 2), ('work', 2), ('accomplishes', 2), ('performing', 2)]    {('group', 24), ('community', 23), ('world', 20), ('race', 3), ('collections', 2), ('groups', 2), ('communities', 1), ('arrangement', 1), ('multitude', 1), }\n",
      "'_role', 35: [('new', 5), ('model', 4), ('lot', 2), ('expanding,', 2), ('leadership', 2), ('challenges', 2), ('truly', 2), ('many', 2), ('role', 1), ('fulfilling', 1)]    {('role', 24), ('roles', 9), ('hats', 2), }\n",
      "'_activities', 90: [('daily', 4), ('people', 4), ('get', 4), ('job', 3), ('able', 3), ('like', 3), ('little', 3), ('good', 3), ('not', 3), ('enjoyment', 3)]    {('activities', 24), ('activity', 17), ('use', 8), ('turn', 8), ('enjoyment', 7), ('writing', 5), ('preparation', 4), ('occupation', 2), ('attempts', 2), ('burst', 2), ('aid', 2), ('bursts', 2), ('laughter', 1), ('provision', 1), ('negotiations', 1), ('aids', 1), ('creation', 1), ('acting', 1), ('search', 1), }\n",
      "'_teamwork', 24: [('teamwork', 12), ('lack', 5), ('no', 2), ('not_enough', 1), ('communication', 1), ('learning', 1), ('apply', 1), ('aspects', 1)]    {('teamwork', 24), }\n",
      "'_government', 43: [('regulations', 8), ('work', 5), ('test', 4), ('local', 3), ('job', 2), ('systems', 2), ('threats', 2), ('agencies', 2), ('tax', 2), ('industry', 2)]    {('government', 24), ('state', 13), ('court', 5), ('states', 1), }\n",
      "'_standards', 29: [('high', 6), ('meet', 3), ('different', 2), ('double', 2), ('unrealistic', 1), ('productivity', 1), ('performance', 1), ('production', 1), ('meeting', 1), ('minimum', 1)]    {('standards', 23), ('standard', 5), ('scale', 1), }\n",
      "'_quota', 30: [('made', 5), ('sales', 3), ('meeting', 3), ('meet', 3), ('hitting', 2), ('quota', 2), ('quotas', 2), ('not_making', 1), ('aggressive', 1), (\"there's\", 1)]    {('quota', 23), ('quotas', 7), }\n",
      "'_comments', 34: [('made', 5), ('no', 5), ('pretty', 2), ('work', 2), ('manager', 2), ('negative', 2), ('bosses', 1), (\"boss'\", 1), ('leave', 1), ('owners', 1)]    {('comments', 23), ('comment', 8), ('observations', 1), ('courtesy', 1), ('observation', 1), }\n",
      "'_attention', 36: [('detail', 4), ('not_pay', 3), ('pay', 3), ('full', 2), ('set', 2), ('requires', 1), ('positive', 1), ('thanked', 1), ('pays', 1), ('capture', 1)]    {('attention', 23), ('notice', 7), ('eyes', 5), ('eye', 1), }\n",
      "'_input', 23: [('no', 3), ('valued', 2), ('lives', 2), ('appraise', 2), ('little', 1), ('value', 1), ('without', 1), ('ask', 1), ('not_asked', 1), ('waiting', 1)]    {('input', 23), }\n",
      "'_manner', 23: [('timely', 17), ('management', 2), ('proper', 2), ('efficient', 1), ('average', 1)]    {('manner', 23), }\n",
      "'_advantage', 63: [('taken', 10), ('new', 7), ('take', 5), ('non', 5), ('early', 5), ('taking', 4), ('want', 3), ('job', 3), ('takes', 2), ('make', 2)]    {('advantage', 22), ('profit', 16), ('start', 15), ('profits', 7), ('privileges', 2), ('privilege', 1), }\n",
      "'_program', 31: [('new', 3), ('computer', 2), ('going', 2), ('not_work', 2), ('ended', 2), ('layouts', 2), ('reward', 1), ('useful', 1), ('fix', 1), ('supervisor', 1)]    {('program', 22), ('programs', 6), ('agenda', 2), ('blueprints', 1), }\n",
      "'_tips', 25: [('good', 5), ('low', 2), ('great', 2), ('big', 2), ('no', 2), ('not', 2), ('compliments', 1), ('tips', 1), ('work', 1), ('shift', 1)]    {('tips', 22), ('tip', 3), }\n",
      "'_technology', 25: [('new', 7), ('keep', 2), ('pace', 2), ('everything', 2), ('works', 2), ('not_function', 1), ('movement', 1), ('technology', 1), ('slow', 1), ('not_up', 1)]    {('technology', 22), ('technologies', 3), }\n",
      "'_peers', 47: [('peers', 7), ('complaining', 2), ('respect', 2), ('surpassed', 2), ('tell', 2), ('customers', 2), ('calling', 2), ('show', 2), ('sales', 2), ('work', 2)]    {('peers', 22), ('associates', 15), ('associate', 5), ('peer', 5), }\n",
      "'_instructions', 32: [('left', 2), ('not_follow', 2), ('poor', 2), ('follow', 2), ('basic', 2), ('conflicting', 2), ('management', 2), ('changes', 2), ('seating', 1), ('different', 1)]    {('instructions', 22), ('instruction', 6), ('style', 2), ('addresses', 1), ('misdirection', 1), }\n",
      "'_contract', 27: [('big', 2), ('contract', 2), ('new', 2), ('work', 2), ('holders', 2), ('award', 2), ('without', 1), ('major', 1), ('securing', 1), ('extended', 1)]    {('contract', 22), ('contracts', 4), ('lease', 1), }\n",
      "'_incompetence', 22: [('incompetence', 8), ('others', 6), ('management', 3), ('boss', 2), (\"coworker's\", 2), ('work', 2), ('coworkers', 2), ('personnel', 2), ('deal', 1)]    {('incompetence', 22), }\n",
      "'_achievement', 33: [('achievement', 7), ('achievements', 7), ('sense', 3), ('goals', 2), ('exceeded', 2), ('goal', 2), ('recognized', 2), ('tasks', 2), ('it´s', 1), ('self', 1)]    {('achievement', 21), ('achievements', 12), }\n",
      "'_fear', 24: [('failure', 4), ('superiors', 4), ('making', 2), ('failure,', 2), ('thrown', 2), ('boss', 2), ('fired', 2), ('loosing', 2), ('mistakes', 2), ('perform', 2)]    {('fear', 21), ('alarm', 1), ('panic', 1), ('intimidation', 1), }\n",
      "'_teacher', 40: [('regular', 2), ('mean', 2), ('annoying', 2), ('got', 2), ('rude', 2), ('rely', 2), ('appreciates', 2), ('lead', 1), ('teacher', 1), ('love', 1)]    {('teacher', 21), ('teachers', 17), ('coach', 2), }\n",
      "'_review', 27: [('performance', 6), ('good', 5), ('annual', 1), ('recruited.', 1), ('not_rewarded', 1), ('excellent', 1), ('bad', 1), ('mediocre', 1), ('exhausting', 1), ('job', 1)]    {('review', 21), ('reviews', 6), }\n",
      "'_art', 26: [('making', 3), ('art', 3), ('subjective', 2), ('work', 2), ('seasonal', 2), ('students', 2), ('sing', 2), ('non-profit', 2), ('connect', 1), ('figure', 1)]    {('art', 21), ('dance', 3), ('arts', 2), }\n",
      "'_nobody', 35: [('cares', 8), ('listens', 2), ('put', 2), ('listen', 2), ('really', 2), ('seems', 2), ('else', 2), ('owner', 2), ('shows', 2), ('tells', 2)]    {('nobody', 20), ('everybody', 12), ('anybody', 2), ('somebody', 1), }\n",
      "'_flexibility', 20: [('flexibility', 4), ('given', 2), ('schedule', 2), ('supervisors', 2), ('make', 2), ('new', 2), ('take', 2), ('work', 2), ('hours', 2), ('provided', 1)]    {('flexibility', 20), }\n",
      "'_opinion', 36: [('ignored', 4), ('asked', 3), ('express', 2), ('voice', 2), ('not_asked', 2), ('valued', 2), ('ot', 2), ('asks', 2), ('not_care', 2), ('taken', 2)]    {('opinion', 20), ('opinions', 14), ('pole', 1), ('judgment', 1), }\n",
      "'_joy', 21: [('joy', 8), ('job', 2), ('gives', 2), ('get', 2), ('real', 2), ('see', 2), ('completing', 2), ('bring', 1), ('brings', 1), ('no', 1)]    {('joy', 20), ('joys', 1), }\n",
      "'_regulations', 23: [('government', 5), ('many', 3), ('rules', 2), ('always', 2), ('leave', 2), ('regulations', 1), ('paperwork', 1), ('fed', 1), ('overtime', 1), ('governmental', 1)]    {('regulations', 20), ('regulation', 3), }\n",
      "'_check', 25: [('pay', 11), ('great', 2), ('called', 1), ('not_get', 1), ('good', 1), ('big', 1), ('check', 1), ('a/p', 1), ('small', 1), ('hours', 1)]    {('check', 20), ('checks', 4), ('checking', 1), }\n",
      "'_morning', 24: [('early', 8), ('work', 4), ('good', 2), ('rush', 2), ('shift', 2), ('every', 2), ('getting', 1), ('time', 1), ('next', 1), ('job', 1)]    {('morning', 19), ('mornings', 5), }\n",
      "'_assistance', 19: [('coworkers', 2), ('recognition/lack', 1), ('needed', 1), ('no', 1), ('adequate', 1), ('gratitude', 1), ('technical', 1), ('guidance', 1), ('requests', 1), ('come', 1)]    {('assistance', 19), }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'_rate', 47: [('pay', 8), ('work', 6), ('boss', 5), ('cash', 4), ('service', 4), ('increases', 2), ('turn', 2), ('going', 2), ('production', 1), ('increased', 1)]    {('rate', 19), ('flow', 15), ('speed', 4), ('jerk', 4), ('jerks', 3), ('rates', 1), ('flows', 1), }\n",
      "'_chance', 20: [('no', 5), ('gave', 3), ('win', 2), ('make', 2), ('people', 1), ('high', 1), ('little', 1), ('took', 1), ('given', 1), ('offered', 1)]    {('chance', 19), ('chances', 1), }\n",
      "'_god', 19: [('god', 5), ('serve', 3), ('called', 2), ('work', 2), ('put', 2), ('good', 2), ('word', 1), ('close', 1), ('saying', 1), ('things', 1)]    {('god', 19), }\n",
      "'_rewards', 30: [('good', 5), ('no', 5), ('rewards', 4), ('little', 2), ('withdrawn', 2), ('job', 2), ('satisfactory', 2), ('getting', 1), ('feel', 1), ('raises', 1)]    {('rewards', 19), ('reward', 11), }\n",
      "'_payday', 19: [('payday', 16), ('arriving', 2), ('disappears', 1), ('come', 1)]    {('payday', 19), }\n",
      "'_plate', 18: [('much', 9), ('lot', 4), ('things', 1), ('put', 1), ('broke', 1), ('work', 1)]    {('plate', 18), }\n",
      "'_age', 20: [('age', 3), ('education', 2), ('young', 2), ('work', 2), ('issue', 2), ('seems', 2), ('children', 1), ('retirement', 1), ('old', 1), ('money', 1)]    {('age', 18), ('ages', 1), ('newness', 1), }\n",
      "'_weight', 18: [('not_pulling', 8), ('not_pull', 6), ('pulls', 1), ('pulling', 1), ('peoples', 1)]    {('weight', 18), }\n",
      "'_energy', 20: [('lot', 7), ('spent', 2), ('no', 2), ('way', 1), ('share', 1), ('energy', 1), ('amount', 1), ('new', 1), ('feel', 1), ('burst', 1)]    {('energy', 18), ('heat', 2), }\n",
      "'_retirement', 18: [('nearing', 2), ('close', 2), ('closer', 1), ('voluntary', 1), ('due', 1), ('save', 1), ('behind', 1), ('approaching', 1), ('till', 1), ('time', 1)]    {('retirement', 18), }\n",
      "'_lunch', 19: [('time', 4), ('lunch', 2), ('break', 2), ('friends', 2), ('made', 1), ('missed', 1), ('not_even', 1), ('forgot', 1), ('not_consistent', 1), ('free', 1)]    {('lunch', 18), ('lunches', 1), }\n",
      "'_choice', 36: [('no', 6), ('right', 5), ('career', 4), ('bad', 3), ('first', 2), ('played', 2), ('freedom', 1), ('field', 1), ('not_a', 1), ('stupid', 1)]    {('choice', 18), ('choices', 12), ('favorites', 6), }\n",
      "'_motivation', 18: [('lack', 3), ('gives', 2), ('student', 2), ('company', 2), ('real', 1), ('providing', 1), ('no', 1), ('not_have', 1), ('received', 1), ('reward', 1)]    {('motivation', 17), ('motivations', 1), }\n",
      "'_requests', 29: [('support', 3), ('made', 2), ('complete', 2), ('vacation', 2), ('chronically', 2), ('unusual', 1), ('process', 1), ('information', 1), ('denial', 1), ('not_listening', 1)]    {('requests', 17), ('request', 5), ('application', 4), ('applications', 3), }\n",
      "'_superiors', 18: [('want', 2), ('flip-flopped', 2), ('fear', 2), ('paid', 2), ('communication', 2), ('superiors', 2), ('seemed', 2), ('great', 2), ('job', 1), ('recognition', 1)]    {('superiors', 17), ('superior', 1), }\n",
      "'_acknowledgement', 17: [('no', 8), ('lack', 3), ('accomplishment', 2), ('without', 1), ('acknowledgement', 1), ('due', 1), ('recognition', 1)]    {('acknowledgement', 17), }\n",
      "'_relationships', 30: [('good', 7), ('others', 6), ('work', 2), ('improving', 2), ('boss', 2), ('means', 1), ('great', 1), ('coworker', 1), ('knowledge', 1), ('colleague', 1)]    {('relationships', 17), ('relationship', 13), }\n",
      "'_outcomes', 61: [('people', 14), ('customers', 8), ('good', 6), ('public', 6), ('great', 5), ('positive', 4), ('rude', 4), ('lot', 4), ('big', 4), ('lots', 4)]    {('outcomes', 17), ('deal', 15), ('outcome', 13), ('deals', 11), ('consequences', 5), }\n",
      "'_leader', 36: [('team', 4), ('poor', 2), ('leader', 2), ('disrespectful', 2), ('shift', 2), ('questions', 2), ('must', 2), ('nasty', 2), ('role', 2), ('voted', 1)]    {('leader', 16), ('leaders', 7), ('trainer', 6), ('callers', 4), ('model', 2), ('caller', 1), }\n",
      "'_anxiety', 17: [('anxiety', 5), ('gives', 2), ('social', 2), ('stressful', 2), ('ridden', 2), ('suffer', 1), ('performance', 1), ('longer', 1), ('starts', 1), ('due', 1)]    {('anxiety', 16), ('anxieties', 1), }\n",
      "'_cuts', 28: [('hours', 23), ('pay', 5), ('job', 4), ('budget', 4), ('employee', 2), ('got', 2), ('time', 2), ('backs', 2), ('department', 2), ('benefits', 2)]    {('cuts', 16), ('cut', 12), }\n",
      "'_events', 30: [('things', 5), ('unexpected', 2), ('big', 2), ('led', 2), ('taking', 2), ('worked', 2), ('coordinators', 2), ('nothing', 2), ('changes', 2), ('not', 2)]    {('events', 16), ('event', 11), ('happening', 2), ('makeup', 1), }\n",
      "'_errors', 23: [('errors', 3), ('made', 3), ('coding', 2), ('payroll', 1), ('feedback', 1), ('writing', 1), ('scheduling', 1), ('discovered', 1), ('accountable', 1), ('critical', 1)]    {('errors', 16), ('error', 7), }\n",
      "'_freedom', 21: [('freedom', 3), ('hours', 2), ('share', 2), ('choice', 2), ('listen', 2), ('self-employment', 2), ('independence', 2), ('enjoy', 1), ('aspect', 1), ('allowed', 1)]    {('freedom', 16), ('independence', 5), }\n",
      "'_ethic', 21: [('work', 17), ('ethics', 1), ('values,', 1), ('excellent', 1)]    {('ethic', 16), ('ethics', 5), }\n",
      "'_college', 16: [('years', 2), ('work', 2), ('graduating', 1), ('local', 1), ('across', 1), ('putting', 1), ('hours', 1), ('finished', 1), ('using', 1), ('going', 1)]    {('college', 16), }\n",
      "'_bit', 15: [('feel', 3), ('little', 2), ('nerve', 2), ('things', 1), ('tiny', 1), ('students', 1), ('tend', 1), ('workload', 1), ('one', 1), ('come', 1)]    {('bit', 15), }\n",
      "'_failure', 25: [('call', 5), ('failure', 3), ('fear', 2), ('pass', 2), ('computer', 1), ('risk', 1), ('equipment', 1), ('not_like', 1), ('alternatives', 1), ('bosses', 1)]    {('failure', 15), ('outs', 6), ('failures', 4), }\n",
      "'_top', 15: [('performers', 2), ('us', 2), ('producer', 2), ('stay', 2), ('class', 2), ('performer', 2), ('game', 2), ('acquired', 1), ('direction', 1), ('pressure', 1)]    {('top', 15), }\n",
      "'_degree', 17: [('field', 3), ('highest', 1), ('choice', 1), ('due', 1), ('bach', 1), ('100', 1), ('not_using', 1), ('advanced', 1), (\"bachelor's\", 1), ('bs', 1)]    {('degree', 15), ('degrees', 2), }\n",
      "'_cases', 28: [('work', 2), ('many', 2), ('medical', 2), ('hard', 2), ('new', 2), ('came', 2), ('promoted', 1), ('gave', 1), ('assigned', 1), ('dealt', 1)]    {('cases', 15), ('case', 7), ('pieces', 3), ('piece', 3), }\n",
      "'_trouble', 31: [('no', 18), ('getting', 5), ('not', 5), ('subject', 5), ('work', 3), ('get', 2), ('employee', 2), ('staying', 2), ('matter', 2), ('something', 2)]    {('trouble', 15), ('matter', 8), ('matters', 5), ('troubles', 3), }\n",
      "'_travel', 29: [('travel', 7), ('time', 6), ('lot', 6), ('without', 4), ('lots', 3), ('truck', 3), ('usa', 2), ('every', 2), ('risks', 2), ('relaxing', 2)]    {('travel', 15), ('driving', 8), ('walk', 4), ('stage', 1), ('journey', 1), }\n",
      "'_testing', 15: [('testing', 7), ('results', 2), ('much', 2), ('school', 1), ('set', 1), ('challenges', 1), ('certain', 1), ('time', 1)]    {('testing', 15), }\n",
      "'_info', 15: [('provide', 2), ('providing', 1), ('give', 1), ('training', 1), ('know', 1), ('customers', 1), ('missing', 1), ('submitting', 1), ('ending', 1), ('not_enough', 1)]    {('info', 15), }\n",
      "'_co', 14: [('manager', 2), ('sized', 1), ('satisfied', 1), ('co', 1), ('great', 1), ('owned', 1)]    {('co', 14), }\n",
      "'_making', 14: [('money', 92), ('difference', 68), ('not', 26), (\"i'm\", 25), ('good', 19), ('people', 15), ('sure', 13), ('like', 12), ('decision', 11), ('others', 9)]    {('making', 14), }\n",
      "'_economy', 15: [('economy', 4), ('bad', 3), ('stinks', 2), ('slow,', 2), ('slow', 2), ('good', 1), ('changing', 1), ('improved', 1), ('live', 1)]    {('economy', 14), ('economies', 1), }\n",
      "'_dream', 18: [('job', 12), ('work', 3), ('field', 2), ('living', 2), ('getting', 1), ('not_my', 1), ('get', 1), ('people', 1), ('achieve', 1), ('see', 1)]    {('dream', 14), ('dreams', 3), ('nightmares', 1), }\n",
      "'_cash', 14: [('flow', 6), ('register', 4), ('get', 2), ('great', 1), ('operate', 1), ('pay', 1), ('stand', 1), ('cash', 1), ('enough', 1), ('managing', 1)]    {('cash', 14), }\n",
      "'_rest', 15: [('not_enough', 4), ('no', 3), ('need', 2), ('staff', 2), ('not_get', 1), ('thing', 1), ('not', 1), ('enough', 1), ('fridays', 1), ('adequate', 1)]    {('rest', 14), ('rests', 1), }\n",
      "'_cooperation', 26: [('lack', 8), ('no', 2), ('deadlines', 2), ('not_having', 1), ('anticipation', 1), ('coworker', 1), ('get', 1), ('cooperation', 1), ('team', 1), ('commitments', 1)]    {('cooperation', 14), ('commitments', 5), ('commitment', 5), ('collaboration', 1), ('compromises', 1), }\n",
      "'_weekend', 24: [('weekend', 7), ('work', 3), ('almost', 2), ('no', 2), ('overtime', 1), ('thanksgiving', 1), ('violence', 1), ('full', 1), ('husky', 1), ('not_working', 1)]    {('weekend', 14), ('weekends', 10), }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'_cause', 19: [('cause', 21), (\"i'm\", 16), ('job', 14), ('got', 5), ('love', 4), ('always', 4), ('good', 4), ('management', 4), ('work', 4), ('get', 3)]    {('cause', 14), ('factors', 3), ('producer', 1), ('causes', 1), }\n",
      "'_power', 34: [('lack', 5), ('no', 4), ('dynamics', 2), ('man', 2), ('trip', 2), ('best', 2), ('position', 1), ('nuclear', 1), ('students', 1), ('power', 1)]    {('power', 14), ('interest', 12), ('interests', 6), ('effectiveness', 2), }\n",
      "'_accountability', 14: [('no', 6), ('lack', 3), ('much', 2), ('without', 1), ('workload', 1), ('levels', 1), ('take', 1)]    {('accountability', 14), }\n",
      "'_complaints', 17: [('customer', 4), ('no', 3), ('complaints', 3), ('job', 2), ('administration', 1), ('lot', 1), ('without', 1), ('work', 1), ('complaint', 1)]    {('complaints', 14), ('complaint', 3), }\n",
      "'_site', 16: [('job', 3), ('disarrayed', 2), ('feedback', 1), ('relocation', 1), ('others', 1), ('meetings', 1), ('employees', 1), ('theft', 1), ('work', 1), ('performing', 1)]    {('site', 14), ('sites', 2), }\n",
      "'_personnel', 17: [('military', 5), ('changes', 2), ('issues', 2), ('qualified', 2), ('always', 2), ('lack', 1), ('not_enough', 1), ('sales', 1), ('level', 1), ('incompetence', 1)]    {('personnel', 14), ('military', 3), }\n",
      "'_value', 21: [('work', 7), ('company', 4), ('dollar', 2), ('not', 2), ('knowledge', 2), ('no', 2), ('values', 2), ('contribution', 1), ('recognized', 1), ('skills', 1)]    {('value', 14), ('values', 6), ('argument', 1), }\n",
      "'_conflict', 28: [('coworker', 10), ('owner', 4), ('conflict', 3), ('no', 2), ('another', 2), ('scheduling', 2), ('heads', 2), ('support', 2), ('nonsenses', 2), ('misbehavior', 2)]    {('conflict', 14), ('conflicts', 7), ('war', 3), ('fight', 2), ('fights', 2), }\n",
      "'_circumstances', 15: [('due', 2), ('unfortunate', 2), ('circumstances', 2), ('turn', 2), ('supervisors', 1), ('factual', 1), ('regardless', 1), ('problematic', 1), ('control', 1), ('difficult', 1)]    {('circumstances', 14), ('circumstance', 1), }\n",
      "'_timelines', 21: [('timelines', 8), ('short', 2), ('tight', 2), ('projects', 2), ('sped', 2), ('meet', 2), ('missed', 2), ('unreasonable', 1), ('scheduling', 1), ('timeline', 1)]    {('timelines', 13), ('timeline', 8), }\n",
      "'_retail', 13: [('store', 2), ('work', 2), ('management', 2), ('hours,', 2), ('sometimes', 2), (\"holiday's\", 2), ('holiday', 2), ('frustrating', 2), ('christmas', 2), ('get', 2)]    {('retail', 13), }\n",
      "'_concerns', 20: [('ignored', 2), ('lack', 2), ('everyone', 2), ('things', 2), ('several', 1), ('health', 1), ('unheeded', 1), ('voiced', 1), ('not_listened', 1), ('shared', 1)]    {('concerns', 13), ('concern', 7), }\n",
      "'_tax', 15: [('senior', 2), ('season', 2), ('deadlines', 2), ('knowledge', 1), ('advanced', 1), ('state', 1), ('preparing', 1), ('entire', 1), ('operating', 1), ('everyone', 1)]    {('tax', 13), ('taxes', 2), }\n",
      "'_favoritism', 13: [('favoritism', 4), ('workplace', 4), ('lazy', 2), ('much', 2), ('injustices', 1), ('shows', 1), ('lot', 1), ('employee', 1)]    {('favoritism', 13), }\n",
      "'_consumers', 14: [('make', 2), ('made', 2), ('consumers', 2), ('sometimes', 2), ('great', 2), ('reach', 2), ('needs', 1), ('directly', 1), ('foreign', 1), ('like', 1)]    {('consumers', 13), ('consumer', 1), }\n",
      "'_number', 27: [('workers', 2), ('rate', 2), ('number', 2), ('big', 2), ('chart', 2), ('fun', 2), ('customers', 2), ('compared', 1), ('large', 1), ('supervise', 1)]    {('number', 13), ('numbers', 9), ('majority', 4), ('minorities', 1), }\n",
      "'_idiot', 23: [('boss', 9), ('manager', 3), ('people', 3), ('dealing', 2), ('users', 2), ('dp', 1), ('quality,', 1), ('like', 1), ('seasonal', 1), ('idiots', 1)]    {('idiot', 13), ('idiots', 10), }\n",
      "'_stability', 12: [('stability', 3), ('employment', 3), ('job', 2), ('money', 1), ('financial', 1), ('economic', 1), ('no', 1)]    {('stability', 12), }\n",
      "'_ownership', 12: [('change', 3), ('new', 2), ('ownership', 2), ('time', 2), ('oversight', 2), ('struggle', 2), ('zero', 1), ('not_take', 1)]    {('ownership', 12), }\n",
      "'_description', 13: [('job', 10), ('pay', 1), ('control', 1)]    {('description', 12), ('specifications', 1), }\n",
      "'_solutions', 20: [('find', 3), ('figured', 2), ('come', 1), ('implement', 1), ('not_asked', 1), ('create', 1), ('not_readily', 1), ('solutions', 1), ('figuring', 1), ('problems', 1)]    {('solutions', 12), ('solution', 8), }\n",
      "'_rush', 15: [('rush', 2), ('get', 2), ('holidays', 2), ('time', 2), ('ends', 2), ('work', 1), ('christmas', 1), ('morning', 1), ('resources,', 1), ('like', 1)]    {('rush', 12), ('rushes', 3), }\n",
      "'_desk', 14: [('sit', 3), ('help', 2), ('job', 2), ('reviews', 2), ('sitting', 1), ('hit', 1), ('dumped', 1), ('decent', 1), ('stuck', 1), ('think', 1)]    {('desk', 12), ('secretary', 2), }\n",
      "'_overwork', 12: [('overwork', 9), ('r/t', 4), ('shifts', 2), ('one', 2), ('like', 1), ('sometimes', 1)]    {('overwork', 12), }\n",
      "'_commission', 26: [('sales', 3), ('job', 2), ('commission', 2), ('structure', 2), ('members', 2), ('pressure,', 2), ('get', 2), ('trustees', 2), ('commissions', 2), ('100%', 1)]    {('commission', 12), ('board', 11), ('commissions', 3), }\n",
      "'_point', 14: [('life', 2), ('others', 1), ('rarely', 1), ('make', 1), ('straight', 1), ('seems', 1), ('one', 1), ('point', 1), ('different', 1), ('nervous', 1)]    {('point', 12), ('points', 2), }\n",
      "'_nurse', 18: [('nurse', 4), ('nasty', 2), ('work', 2), (\"i'm\", 1), ('charge', 1), ('treats', 1), ('new', 1), ('nurses.', 1), ('pay', 1), ('one', 1)]    {('nurse', 12), ('nurses', 6), }\n",
      "'_personality', 16: [('different', 3), ('personality', 2), ('conflicts', 2), ('no', 1), ('flaunt', 1), ('negative', 1), ('fits', 1), ('build', 1), (\"that's\", 1), ('skills', 1)]    {('personality', 12), ('personalities', 4), }\n",
      "'_uncertainty', 16: [('uncertainty', 3), ('funding', 2), ('job', 2), ('regarding', 2), ('company', 2), ('unpredictability', 2), ('deal', 1), ('industry', 1), ('income', 1), ('due', 1)]    {('uncertainty', 12), ('unpredictability', 2), ('uncertainties', 2), }\n",
      "'_illness', 18: [('colleague', 2), ('illness', 1), ('mental', 1), ('due', 1), ('related', 1), (\"client's\", 1), ('weather', 1), ('death', 1), ('people', 1), ('dealing', 1)]    {('illness', 12), ('illnesses', 3), ('disease', 2), ('diseases', 1), }\n",
      "'_teenagers', 12: [('work', 3), ('overwhelming', 2), ('attitude', 2), ('great', 1), ('coaching', 1), ('depending', 1), ('help', 1), ('working', 1), ('teenagers', 1), ('problems', 1)]    {('teenagers', 11), ('teenager', 1), }\n",
      "'_research', 11: [('research', 4), ('new', 3), ('project', 2), ('forced', 2), ('kind', 1), ('paralegal', 1), ('taken', 1), ('original', 1), ('cancer', 1)]    {('research', 11), }\n",
      "'_facility', 26: [('wide', 3), ('new', 2), ('allows', 2), ('work', 2), ('income', 2), ('cleaning', 1), ('using', 1), ('waiting', 1), ('known', 1), ('local', 1)]    {('facility', 11), ('range', 4), ('source', 4), ('station', 3), ('stations', 2), ('facilities', 1), ('sources', 1), }\n",
      "'_winter', 11: [('time', 4), ('slow', 3), ('hours', 2), ('weather', 2), ('normal', 1), ('took', 1), ('ready', 1), ('lighter', 1), ('nothing', 1), ('cash', 1)]    {('winter', 11), }\n",
      "'_leave', 11: [('work', 7), ('want', 4), ('ready', 3), ('medical', 2), ('maternity', 2), ('wanted', 2), ('due', 2), ('husband', 2), ('time', 2), ('sick', 2)]    {('leave', 11), }\n",
      "'_award', 17: [('given', 2), ('year', 1), ('receiving', 1), ('major', 1), ('earning', 1), ('annual', 1), ('contract', 1), ('nominated', 1), ('top', 1), ('peer', 1)]    {('award', 11), ('awards', 6), }\n",
      "'_ceo', 12: [('bark', 2), ('guessed', 1), ('degrading', 1), ('ceo', 1), ('not_care', 1), ('boss', 1), ('insults', 1), ('met', 1), ('new', 1), ('checking', 1)]    {('ceo', 11), ('ceos', 1), }\n",
      "'_faces', 17: [('kids', 4), ('smiling', 2), ('happy', 2), ('pace', 2), ('brought', 1), ('peoples', 1), ('smiles', 1), ('smile', 1), ('surprise', 1), ('seeing', 1)]    {('faces', 11), ('face', 6), }\n",
      "'_autonomy', 11: [('autonomy', 4), ('lack', 2), ('complete', 2), ('previous', 1), ('want', 1), ('not_enough', 1), ('responsibility', 1)]    {('autonomy', 11), }\n",
      "'_risk', 15: [('risk', 4), ('failure', 2), ('inherent', 1), ('increases', 1), ('gamble', 1), ('procedures', 1), ('lots', 1), ('financial', 1), ('risks', 1), ('travel', 1)]    {('risk', 11), ('risks', 4), }\n",
      "'_term', 17: [('long', 4), ('done', 2), ('short', 1), ('good', 1), ('firm', 1), ('behind', 1), ('busy', 1), ('stability', 1), ('much', 1)]    {('term', 11), ('terms', 6), }\n",
      "'_cleaning', 11: [('cleaning', 3), ('bathrooms', 2), ('facility', 2), (\"people's\", 2), ('area', 2), ('employee', 2), ('entire', 2), ('correct', 1), ('like', 1), ('routine', 1)]    {('cleaning', 11), }\n",
      "'_understanding', 12: [('not', 6), ('boss', 5), ('lack', 4), ('time', 2), ('scoring', 2), ('difficulty', 1), ('lacks', 1), ('not_really', 1), ('managers', 1), ('help', 1)]    {('understanding', 11), ('realization', 1), }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'_constraints', 13: [('time', 13)]    {('constraints', 11), ('constraint', 2), }\n",
      "'_supplies', 13: [('enough', 2), ('shortage', 2), ('cleaning', 1), ('lot', 1), ('needed', 1), ('running', 1), ('lack', 1), ('safety', 1), ('not_get', 1), ('not_enough', 1)]    {('supplies', 11), ('supply', 2), }\n",
      "'_incentives', 13: [('no', 4), ('went', 2), ('taken', 2), ('finishing', 2), ('good', 1), ('us', 1), ('many', 1), ('people', 1), ('incentives', 1), ('praise', 1)]    {('incentives', 11), ('incentive', 2), }\n",
      "'_planning', 11: [('special', 2), ('organization', 2), ('switch', 2), ('planning', 2), ('tax', 1), ('much', 1), ('case', 1), ('not_enough', 1), ('good', 1), ('poor', 1)]    {('planning', 11), }\n",
      "'_obstacles', 13: [('overcome', 3), ('sometimes', 2), ('weather', 2), ('way', 2), ('could', 2), ('many', 1), ('numerous', 1), ('overcoming', 1), ('ran', 1), ('barriers', 1)]    {('obstacles', 11), ('barriers', 2), }\n",
      "'_tape', 11: [('red', 11)]    {('tape', 11), }\n",
      "'_road', 24: [('drivers', 4), ('much', 4), ('lot', 4), ('time', 3), ('blocks', 2), ('kill', 2), ('driving', 2), ('different', 2), ('whole', 2), ('traffic', 2)]    {('road', 11), ('drive', 7), ('track', 5), ('highway', 1), }\n"
     ]
    }
   ],
   "source": [
    "print_words_associated_with_common_noun_groups(\n",
    "    nlp, \"Yale survey emotion reasons (FEELATWORK_TASKS_<n>_WHY_clean's)\", all_reasons['all_reasons_concat'],\n",
    "    exclude_words, 300, 10, 100, 1, 0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed 29 Aug 11:35:24 BST 2018\r\n"
     ]
    }
   ],
   "source": [
    "!date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
